{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<p>\n",
    "DonorsChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. Right now, a large number of volunteers is needed to manually screen each submission before it's approved to be posted on the DonorsChoose.org website.\n",
    "</p>\n",
    "<p>\n",
    "    Next year, DonorsChoose.org expects to receive close to 500,000 project proposals. As a result, there are three main problems they need to solve:\n",
    "<ul>\n",
    "<li>\n",
    "    How to scale current manual processes and resources to screen 500,000 projects so that they can be posted as quickly and as efficiently as possible</li>F\n",
    "    <li>How to increase the consistency of project vetting across different volunteers to improve the experience for teachers</li>\n",
    "    <li>How to focus volunteer time on the applications that need the most assistance</li>\n",
    "    </ul>\n",
    "</p>    \n",
    "<p>\n",
    "The goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the Essay Data\n",
    "\n",
    "<ul>\n",
    "Prior to May 17, 2016, the prompts for the essays were as follows:\n",
    "<li>__project_essay_1:__ \"Introduce us to your classroom\"</li>\n",
    "<li>__project_essay_2:__ \"Tell us more about your students\"</li>\n",
    "<li>__project_essay_3:__ \"Describe how your students will use the materials you're requesting\"</li>\n",
    "<li>__project_essay_3:__ \"Close by sharing why your project will make a difference\"</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "Starting on May 17, 2016, the number of essays was reduced from 4 to 2, and the prompts for the first 2 essays were changed to the following:<br>\n",
    "<li>__project_essay_1:__ \"Describe your students: What makes your students special? Specific details about their background, your neighborhood, and your school are all helpful.\"</li>\n",
    "<li>__project_essay_2:__ \"About your project: How will these materials make a difference in your students' learning and improve their school lives?\"</li>\n",
    "<br>For all projects with project_submitted_datetime of 2016-05-17 and later, the values of project_essay_3 and project_essay_4 will be NaN.\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the necessary Libraries\n",
    "we will need to import libraries that allow for data analysis and data visualization to get acclimated to the dataset. We will be using pandas, numpy, matplotlib and seaborn to conduct this. Data Exploration libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "warnings.filterwarnings(\"ignore\",'detected Windows; aliasing chunkize to chunkize_serial')\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read in the dataset.\n",
    "We will use the pandas .read_csv() method to read in the dataset. Then we will use the. head() method to observe the first few rows of the data, to understand the information better. In our case, the feature(column) headers tell us pretty little. This is fine because we are merely trying to gain insight via classifying new data points by referencing it’s neighboring elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p233245</td>\n",
       "      <td>LC652 - Lakeshore Double-Space Mobile Drying Rack</td>\n",
       "      <td>1</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                        description  quantity  price\n",
       "0  p233245  LC652 - Lakeshore Double-Space Mobile Drying Rack         1  149.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_data = pd.read_csv(\"C:\\\\VipinML\\\\Assignment 2\\\\Assignments_DonorsChoose_2018\\\\train_data.csv\")\n",
    "resource_data = pd.read_csv(\"C:\\\\VipinML\\Assignment 2\\\\Assignments_DonorsChoose_2018\\\\resources.csv\")\n",
    "#Limit the data for testing purpose since processing takes few hours for full set..\n",
    "\n",
    "project_data = project_data.head(2000)\n",
    "resource_data = resource_data.head (2000)\n",
    "\n",
    "resource_data.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in train data (2000, 17)\n",
      "--------------------------------------------------\n",
      "The attributes of data : ['Unnamed: 0' 'id' 'teacher_id' 'teacher_prefix' 'school_state'\n",
      " 'project_submitted_datetime' 'project_grade_category'\n",
      " 'project_subject_categories' 'project_subject_subcategories'\n",
      " 'project_title' 'project_essay_1' 'project_essay_2' 'project_essay_3'\n",
      " 'project_essay_4' 'project_resource_summary'\n",
      " 'teacher_number_of_previously_posted_projects' 'project_is_approved']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points in train data\", project_data.shape)\n",
    "print('-'*50)\n",
    "print(\"The attributes of data :\", project_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>Date</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>100660</td>\n",
       "      <td>p234804</td>\n",
       "      <td>cbc0e38f522143b86d372f8b43d4cff3</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>GA</td>\n",
       "      <td>2016-04-27 00:53:00</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Applied Learning</td>\n",
       "      <td>Early Development</td>\n",
       "      <td>Flexible Seating for Flexible Learning</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>I teach at a low-income (Title 1) school. Ever...</td>\n",
       "      <td>We need a classroom rug that we can use as a c...</td>\n",
       "      <td>Benjamin Franklin once said, \\\"Tell me and I f...</td>\n",
       "      <td>My students need flexible seating in the class...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       id                        teacher_id teacher_prefix  \\\n",
       "473      100660  p234804  cbc0e38f522143b86d372f8b43d4cff3           Mrs.   \n",
       "\n",
       "    school_state                Date project_grade_category  \\\n",
       "473           GA 2016-04-27 00:53:00          Grades PreK-2   \n",
       "\n",
       "    project_subject_categories project_subject_subcategories  \\\n",
       "473           Applied Learning             Early Development   \n",
       "\n",
       "                              project_title  \\\n",
       "473  Flexible Seating for Flexible Learning   \n",
       "\n",
       "                                       project_essay_1  \\\n",
       "473  I recently read an article about giving studen...   \n",
       "\n",
       "                                       project_essay_2  \\\n",
       "473  I teach at a low-income (Title 1) school. Ever...   \n",
       "\n",
       "                                       project_essay_3  \\\n",
       "473  We need a classroom rug that we can use as a c...   \n",
       "\n",
       "                                       project_essay_4  \\\n",
       "473  Benjamin Franklin once said, \\\"Tell me and I f...   \n",
       "\n",
       "                              project_resource_summary  \\\n",
       "473  My students need flexible seating in the class...   \n",
       "\n",
       "     teacher_number_of_previously_posted_projects  project_is_approved  \n",
       "473                                             2                    1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to replace elements in list python: https://stackoverflow.com/a/2582163/4084039\n",
    "cols = ['Date' if x=='project_submitted_datetime' else x for x in list(project_data.columns)]\n",
    "#sort dataframe based on time pandas python: https://stackoverflow.com/a/49702492/4084039\n",
    "project_data['Date'] = pd.to_datetime(project_data['project_submitted_datetime'])\n",
    "project_data.drop('project_submitted_datetime', axis=1, inplace=True)\n",
    "project_data.sort_values(by=['Date'], inplace=True)\n",
    "\n",
    "# how to reorder columns pandas python: https://stackoverflow.com/a/13148611/4084039\n",
    "project_data = project_data[cols]\n",
    "project_data.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 preprocessing of `project_subject_categories`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "catogories = list(project_data['project_subject_categories'].values)\n",
    "# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n",
    "\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n",
    "# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n",
    "cat_list = []\n",
    "for i in catogories:\n",
    "    temp = \"\"\n",
    "    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n",
    "    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n",
    "        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n",
    "            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n",
    "        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n",
    "        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n",
    "        temp = temp.replace('&','_') # we are replacing the & value into \n",
    "    cat_list.append(temp.strip())\n",
    "    \n",
    "project_data['clean_categories'] = cat_list\n",
    "project_data.drop(['project_subject_categories'], axis=1, inplace=True)\n",
    "\n",
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "for word in project_data['clean_categories'].values:\n",
    "    my_counter.update(word.split())\n",
    "\n",
    "cat_dict = dict(my_counter)\n",
    "sorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 preprocessing of `project_subject_subcategories`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_catogories = list(project_data['project_subject_subcategories'].values)\n",
    "# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n",
    "\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n",
    "# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n",
    "\n",
    "sub_cat_list = []\n",
    "for i in sub_catogories:\n",
    "    temp = \"\"\n",
    "    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n",
    "    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n",
    "        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n",
    "            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n",
    "        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n",
    "        temp +=j.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n",
    "        temp = temp.replace('&','_')\n",
    "    sub_cat_list.append(temp.strip())\n",
    "\n",
    "project_data['clean_subcategories'] = sub_cat_list\n",
    "project_data.drop(['project_subject_subcategories'], axis=1, inplace=True)\n",
    "\n",
    "# count of all the words in corpus python: https://stackoverflow.com/a/22898595/4084039\n",
    "my_counter = Counter()\n",
    "for word in project_data['clean_subcategories'].values:\n",
    "    my_counter.update(word.split())\n",
    "    \n",
    "sub_cat_dict = dict(my_counter)\n",
    "sorted_sub_cat_dict = dict(sorted(sub_cat_dict.items(), key=lambda kv: kv[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_cat = list(project_data['teacher_prefix'].values)\n",
    "# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n",
    "\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n",
    "# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n",
    "cat_list = []\n",
    "for i in teacher_cat:\n",
    "    temp = \"\"\n",
    "    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n",
    "    j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n",
    "    temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n",
    "    temp = temp.replace('&','_') # we are replacing the & value into \n",
    "    cat_list.append(temp.strip())\n",
    "\n",
    "project_data.drop(['teacher_prefix'], axis=1, inplace=True)\n",
    "project_data['teacher_prefix'] = sub_cat_list\n",
    "    \n",
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "for word in project_data['teacher_prefix'].values:\n",
    "    my_counter.update(word.split())\n",
    "\n",
    "cat_dict = dict(my_counter)\n",
    "sorted_teacher_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two column text dataframe: \n",
    "project_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n",
    "                        project_data[\"project_essay_2\"].map(str) + \\\n",
    "                        project_data[\"project_essay_3\"].map(str) + \\\n",
    "                        project_data[\"project_essay_4\"].map(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean project_grade_category\n",
    "project_data[\"project_grade_category\"] = \\\n",
    "project_data.apply(lambda x: (x['project_grade_category'].replace(' ', '_')), axis=1)\n",
    "project_data[\"project_grade_category\"] = \\\n",
    "project_data.apply(lambda x: (x['project_grade_category'].replace('-', '_')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>school_state</th>\n",
       "      <th>Date</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>100660</td>\n",
       "      <td>p234804</td>\n",
       "      <td>cbc0e38f522143b86d372f8b43d4cff3</td>\n",
       "      <td>GA</td>\n",
       "      <td>2016-04-27 00:53:00</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>Flexible Seating for Flexible Learning</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>I teach at a low-income (Title 1) school. Ever...</td>\n",
       "      <td>We need a classroom rug that we can use as a c...</td>\n",
       "      <td>Benjamin Franklin once said, \\\"Tell me and I f...</td>\n",
       "      <td>My students need flexible seating in the class...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AppliedLearning</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       id                        teacher_id school_state  \\\n",
       "473      100660  p234804  cbc0e38f522143b86d372f8b43d4cff3           GA   \n",
       "\n",
       "                   Date project_grade_category  \\\n",
       "473 2016-04-27 00:53:00          Grades_PreK_2   \n",
       "\n",
       "                              project_title  \\\n",
       "473  Flexible Seating for Flexible Learning   \n",
       "\n",
       "                                       project_essay_1  \\\n",
       "473  I recently read an article about giving studen...   \n",
       "\n",
       "                                       project_essay_2  \\\n",
       "473  I teach at a low-income (Title 1) school. Ever...   \n",
       "\n",
       "                                       project_essay_3  \\\n",
       "473  We need a classroom rug that we can use as a c...   \n",
       "\n",
       "                                       project_essay_4  \\\n",
       "473  Benjamin Franklin once said, \\\"Tell me and I f...   \n",
       "\n",
       "                              project_resource_summary  \\\n",
       "473  My students need flexible seating in the class...   \n",
       "\n",
       "     teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "473                                             2                    1   \n",
       "\n",
       "    clean_categories clean_subcategories    teacher_prefix  \\\n",
       "473  AppliedLearning    EarlyDevelopment  EarlyDevelopment   \n",
       "\n",
       "                                                 essay  \n",
       "473  I recently read an article about giving studen...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.4.2.3 Using Pretrained Models: TFIDF weighted W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s a teacher at a Title 1 school, many of my students receive free or reduced price lunch and have extremely limited resources.  Some face challenges daily that many adults never have to experience. M\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "sent = decontracted(project_data['essay'].values[500])\n",
    "print(sent[1:200])\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s a teacher at a Title 1 school, many of my students receive free or reduced price lunch and have extremely limited resources.  Some face challenges daily that many adults never have to experience. M\n",
      "s a teacher at a Title 1 school, many of my students receive free or reduced price lunch and have extremely limited resources.  Some face challenges daily that many adults never have to experience. M\n"
     ]
    }
   ],
   "source": [
    "# \\r \\n \\t remove from string python: http://texthandler.com/info/remove-line-breaks-python/\n",
    "sent = sent.replace('\\\\r', ' ')\n",
    "sent = sent.replace('\\\\\"', ' ')\n",
    "sent = sent.replace('\\\\n', ' ')\n",
    "print(sent[1:200])\n",
    "print(sent[1:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s a teacher at a Title 1 school many of my students receive free or reduced price lunch and have extremely limited resources Some face challenges daily that many adults never have to experience My st\n"
     ]
    }
   ],
   "source": [
    "#remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
    "sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "print(sent[1:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Merging price with project_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id   price  quantity\n",
      "1  p003483   81.28        40\n",
      "2  p006068  557.98        16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>school_state</th>\n",
       "      <th>Date</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100660</td>\n",
       "      <td>p234804</td>\n",
       "      <td>cbc0e38f522143b86d372f8b43d4cff3</td>\n",
       "      <td>GA</td>\n",
       "      <td>2016-04-27 00:53:00</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>Flexible Seating for Flexible Learning</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>I teach at a low-income (Title 1) school. Ever...</td>\n",
       "      <td>We need a classroom rug that we can use as a c...</td>\n",
       "      <td>Benjamin Franklin once said, \\\"Tell me and I f...</td>\n",
       "      <td>My students need flexible seating in the class...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AppliedLearning</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                        teacher_id school_state  \\\n",
       "0      100660  p234804  cbc0e38f522143b86d372f8b43d4cff3           GA   \n",
       "\n",
       "                 Date project_grade_category  \\\n",
       "0 2016-04-27 00:53:00          Grades_PreK_2   \n",
       "\n",
       "                            project_title  \\\n",
       "0  Flexible Seating for Flexible Learning   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  I recently read an article about giving studen...   \n",
       "\n",
       "                                     project_essay_2  \\\n",
       "0  I teach at a low-income (Title 1) school. Ever...   \n",
       "\n",
       "                                     project_essay_3  \\\n",
       "0  We need a classroom rug that we can use as a c...   \n",
       "\n",
       "                                     project_essay_4  \\\n",
       "0  Benjamin Franklin once said, \\\"Tell me and I f...   \n",
       "\n",
       "                            project_resource_summary  \\\n",
       "0  My students need flexible seating in the class...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                             2                    1   \n",
       "\n",
       "  clean_categories clean_subcategories    teacher_prefix  \\\n",
       "0  AppliedLearning    EarlyDevelopment  EarlyDevelopment   \n",
       "\n",
       "                                               essay  price  quantity  \n",
       "0  I recently read an article about giving studen...    NaN       NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\n",
    "project_data = pd.merge(project_data, price_data, on='id', how='left')\n",
    "print (price_data[1:3])\n",
    "project_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3.1 Merge Project Title Count with project_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>school_state</th>\n",
       "      <th>Date</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>...</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>project_title_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100660</td>\n",
       "      <td>p234804</td>\n",
       "      <td>cbc0e38f522143b86d372f8b43d4cff3</td>\n",
       "      <td>GA</td>\n",
       "      <td>2016-04-27 00:53:00</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>Flexible Seating for Flexible Learning</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>I teach at a low-income (Title 1) school. Ever...</td>\n",
       "      <td>We need a classroom rug that we can use as a c...</td>\n",
       "      <td>...</td>\n",
       "      <td>My students need flexible seating in the class...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AppliedLearning</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                        teacher_id school_state  \\\n",
       "0      100660  p234804  cbc0e38f522143b86d372f8b43d4cff3           GA   \n",
       "\n",
       "                 Date project_grade_category  \\\n",
       "0 2016-04-27 00:53:00          Grades_PreK_2   \n",
       "\n",
       "                            project_title  \\\n",
       "0  Flexible Seating for Flexible Learning   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  I recently read an article about giving studen...   \n",
       "\n",
       "                                     project_essay_2  \\\n",
       "0  I teach at a low-income (Title 1) school. Ever...   \n",
       "\n",
       "                                     project_essay_3  ...  \\\n",
       "0  We need a classroom rug that we can use as a c...  ...   \n",
       "\n",
       "                            project_resource_summary  \\\n",
       "0  My students need flexible seating in the class...   \n",
       "\n",
       "  teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            2                    1   \n",
       "\n",
       "   clean_categories clean_subcategories    teacher_prefix  \\\n",
       "0   AppliedLearning    EarlyDevelopment  EarlyDevelopment   \n",
       "\n",
       "                                               essay price  quantity  \\\n",
       "0  I recently read an article about giving studen...   NaN       NaN   \n",
       "\n",
       "   project_title_count  \n",
       "0                    5  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add count (total number of words) in Project Title in each row.\n",
    "\n",
    "project_title_count = project_data['project_title'].str.split().str.len()\n",
    "project_data['project_title_count'] = project_title_count\n",
    "project_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3.2 Essay count of words for each row and  merge with project_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>school_state</th>\n",
       "      <th>Date</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>...</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>project_title_count</th>\n",
       "      <th>essay_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100660</td>\n",
       "      <td>p234804</td>\n",
       "      <td>cbc0e38f522143b86d372f8b43d4cff3</td>\n",
       "      <td>GA</td>\n",
       "      <td>2016-04-27 00:53:00</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>Flexible Seating for Flexible Learning</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>I teach at a low-income (Title 1) school. Ever...</td>\n",
       "      <td>We need a classroom rug that we can use as a c...</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AppliedLearning</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                        teacher_id school_state  \\\n",
       "0      100660  p234804  cbc0e38f522143b86d372f8b43d4cff3           GA   \n",
       "\n",
       "                 Date project_grade_category  \\\n",
       "0 2016-04-27 00:53:00          Grades_PreK_2   \n",
       "\n",
       "                            project_title  \\\n",
       "0  Flexible Seating for Flexible Learning   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  I recently read an article about giving studen...   \n",
       "\n",
       "                                     project_essay_2  \\\n",
       "0  I teach at a low-income (Title 1) school. Ever...   \n",
       "\n",
       "                                     project_essay_3  ...  \\\n",
       "0  We need a classroom rug that we can use as a c...  ...   \n",
       "\n",
       "  teacher_number_of_previously_posted_projects project_is_approved  \\\n",
       "0                                            2                   1   \n",
       "\n",
       "   clean_categories  clean_subcategories    teacher_prefix  \\\n",
       "0   AppliedLearning     EarlyDevelopment  EarlyDevelopment   \n",
       "\n",
       "                                               essay price quantity  \\\n",
       "0  I recently read an article about giving studen...   NaN      NaN   \n",
       "\n",
       "   project_title_count  essay_count  \n",
       "0                    5          225  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add count (total number of words) in essay in each row.\n",
    "\n",
    "essay_count = project_data['essay'].str.split().str.len()\n",
    "project_data['essay_count'] = essay_count\n",
    "project_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>school_state</th>\n",
       "      <th>Date</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>...</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>project_title_count</th>\n",
       "      <th>essay_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100660</td>\n",
       "      <td>p234804</td>\n",
       "      <td>cbc0e38f522143b86d372f8b43d4cff3</td>\n",
       "      <td>GA</td>\n",
       "      <td>2016-04-27 00:53:00</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>Flexible Seating for Flexible Learning</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>I teach at a low-income (Title 1) school. Ever...</td>\n",
       "      <td>We need a classroom rug that we can use as a c...</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AppliedLearning</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>103.01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                        teacher_id school_state  \\\n",
       "0      100660  p234804  cbc0e38f522143b86d372f8b43d4cff3           GA   \n",
       "\n",
       "                 Date project_grade_category  \\\n",
       "0 2016-04-27 00:53:00          Grades_PreK_2   \n",
       "\n",
       "                            project_title  \\\n",
       "0  Flexible Seating for Flexible Learning   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  I recently read an article about giving studen...   \n",
       "\n",
       "                                     project_essay_2  \\\n",
       "0  I teach at a low-income (Title 1) school. Ever...   \n",
       "\n",
       "                                     project_essay_3  ...  \\\n",
       "0  We need a classroom rug that we can use as a c...  ...   \n",
       "\n",
       "  teacher_number_of_previously_posted_projects project_is_approved  \\\n",
       "0                                            2                   1   \n",
       "\n",
       "   clean_categories  clean_subcategories    teacher_prefix  \\\n",
       "0   AppliedLearning     EarlyDevelopment  EarlyDevelopment   \n",
       "\n",
       "                                               essay   price quantity  \\\n",
       "0  I recently read an article about giving studen...  103.01      4.0   \n",
       "\n",
       "   project_title_count  essay_count  \n",
       "0                    5          225  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert NaN value to mean of the column\n",
    "project_data.fillna(project_data.mean(), inplace=True)\n",
    "project_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into Train and cross validation(or test): Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = project_data['project_is_approved'].values\n",
    "X = project_data.drop(['project_is_approved'], axis=1)\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "catogories_essay = list(project_data['essay'].values)\n",
    "# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n",
    "\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n",
    "# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n",
    "cat_essay_list = []\n",
    "for i in catogories_essay:\n",
    "    temp = \"\"\n",
    "    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n",
    "    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n",
    "        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n",
    "            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n",
    "        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n",
    "        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n",
    "        temp = temp.replace('&','_') # we are replacing the & value into \n",
    "    cat_essay_list.append(temp.strip())\n",
    "    \n",
    "project_data['clean_essay'] = cat_essay_list\n",
    "\n",
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "for word in project_data['clean_essay'].values:\n",
    "    my_counter.update(word.split())\n",
    "\n",
    "cat_essay_dict = dict(my_counter)\n",
    "sorted_cat_essay_dict = dict(sorted(cat_essay_dict.items(), key=lambda kv: kv[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "catogories_title = list(project_data['project_title'].values)\n",
    "# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n",
    "# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n",
    "project_title_list = []\n",
    "for i in catogories_title:\n",
    "    temp = \"\"\n",
    "    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n",
    "    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n",
    "        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n",
    "            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n",
    "        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n",
    "        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n",
    "        temp = temp.replace('&','_') # we are replacing the & value into \n",
    "    project_title_list.append(temp.strip())\n",
    "\n",
    "#project_data.drop('project_title', axis=1, inplace=True)\n",
    "#project_data['project_title'] = project_title_list\n",
    "\n",
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "for word in project_data['project_title'].values:\n",
    "    my_counter.update(word.split())\n",
    "\n",
    "project_title_dict = dict(my_counter)\n",
    "sorted_project_title_dict = dict(sorted(project_title_dict.items(), key=lambda kv: kv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1340/1340 [00:00<00:00, 1753.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "from tqdm import tqdm\n",
    "X_train_preprocessed_essays = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(X_train['essay'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    X_train_preprocessed_essays.append(sent.lower().strip())\n",
    "   # print (X_train_preprocessed_essays)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:00<00:00, 1835.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "from tqdm import tqdm\n",
    "X_test_preprocessed_essays = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(X_test['essay'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    X_test_preprocessed_essays.append(sent.lower().strip())\n",
    "   # print (X_test_preprocessed_essays)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Standardize (normalize) the data scale to prep for Logistic regression.\n",
    "Because the distance between pairs of points plays a critical part on the classification, it is necessary to normalize the data This will generate an array of values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Vectorizing Categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/handling-categorical-and-numerical-features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of clean_categories for X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AppliedLearning', 'AppliedLearning Literacy_Language', 'AppliedLearning SpecialNeeds', 'Care_Hunger', 'Health_Sports', 'Health_Sports Literacy_Language', 'Health_Sports SpecialNeeds', 'History_Civics', 'History_Civics Literacy_Language', 'Literacy_Language', 'Literacy_Language History_Civics', 'Literacy_Language Math_Science', 'Literacy_Language Music_Arts', 'Literacy_Language SpecialNeeds', 'Math_Science', 'Math_Science AppliedLearning', 'Math_Science Literacy_Language', 'Math_Science Music_Arts', 'Math_Science SpecialNeeds', 'Music_Arts', 'SpecialNeeds', 'Warmth', 'Warmth Care_Hunger']\n",
      "Shape of matrix X_train_categories_one_hot  after one hot encodig  (1340, 23)\n",
      "Shape of matrix X_test_categories_one_hot after one hot encodig  (660, 23)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=10,ngram_range=(1,4), max_features=5000,lowercase=False, binary=True)\n",
    "X_train_categories_one_hot = vectorizer.fit_transform(X_train['clean_categories'].values)\n",
    "X_test_categories_one_hot = vectorizer.transform(X_test['clean_categories'].values)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape of matrix X_train_categories_one_hot  after one hot encodig \",X_train_categories_one_hot.shape)\n",
    "print(\"Shape of matrix X_test_categories_one_hot after one hot encodig \",X_test_categories_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vectorization of project_grade_category for X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Grades_3_5', 'Grades_6_8', 'Grades_9_12', 'Grades_PreK_2']\n",
      "Shape of matrix X_train_project_grade_category_one_hot  after one hot encodig  (1340, 4)\n",
      "Shape of matrix X_test_project_grade_category_one_hot after one hot encodig  (660, 4)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=10,ngram_range=(1,4), max_features=5000, lowercase=False, binary=True)\n",
    "X_train_project_grade_category_one_hot = vectorizer.fit_transform(X_train['project_grade_category'].values)\n",
    "X_test_project_grade_category_one_hot = vectorizer.transform(X_test['project_grade_category'].values)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape of matrix X_train_project_grade_category_one_hot  after one hot encodig \",X_train_project_grade_category_one_hot.shape)\n",
    "print(\"Shape of matrix X_test_project_grade_category_one_hot after one hot encodig \",X_test_project_grade_category_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'FL', 'GA', 'IL', 'IN', 'KY', 'LA', 'MA', 'MD', 'MI', 'MN', 'MO', 'NC', 'NJ', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'SC', 'TN', 'TX', 'UT', 'VA', 'WA', 'WI']\n",
      "Shape of matrix X_train_school_state_one_hot  after one hot encodig  (1340, 32)\n",
      "Shape of matrix X_test_school_state_one_hot after one hot encodig  (660, 32)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=10,ngram_range=(1,4), max_features=5000, lowercase=False, binary=True)\n",
    "X_train_school_state_one_hot = vectorizer.fit_transform(X_train['school_state'].values)\n",
    "X_test_school_state_one_hot = vectorizer.transform(X_test['school_state'].values)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape of matrix X_train_school_state_one_hot  after one hot encodig \",X_train_school_state_one_hot.shape)\n",
    "print(\"Shape of matrix X_test_school_state_one_hot after one hot encodig \",X_test_school_state_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of clean_subcategories for X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Economics', 'ForeignLanguages', 'FinancialLiteracy', 'Extracurricular', 'CommunityService', 'ParentInvolvement', 'Civics_Government', 'Warmth', 'Care_Hunger', 'NutritionEducation', 'SocialSciences', 'CharacterEducation', 'History_Geography', 'PerformingArts', 'TeamSports', 'College_CareerPrep', 'Other', 'Music', 'ESL', 'EarlyDevelopment', 'Health_LifeScience', 'Gym_Fitness', 'VisualArts', 'EnvironmentalScience', 'AppliedSciences', 'Health_Wellness', 'SpecialNeeds', 'Literature_Writing', 'Mathematics', 'Literacy']\n",
      "Shape of matrix X_train_sub_categories_one_hot  after one hot encodig  (1340, 30)\n",
      "Shape of matrix X_test_sub_categories_one_hot after oneX_test_sub_categories_one_hot  hot encodig  (660, 30)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one \n",
    "vectorizer = CountVectorizer(min_df=10,ngram_range=(1,4), max_features=5000,vocabulary=list(sorted_sub_cat_dict.keys()), lowercase=False, binary=True)\n",
    "X_train_sub_categories_one_hot = vectorizer.fit_transform(X_train['clean_subcategories'].values)\n",
    "X_test_sub_categories_one_hot = vectorizer.transform(X_test['clean_subcategories'].values)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape of matrix X_train_sub_categories_one_hot  after one hot encodig \",X_train_sub_categories_one_hot.shape)\n",
    "print(\"Shape of matrix X_test_sub_categories_one_hot after oneX_test_sub_categories_one_hot  hot encodig \",X_test_sub_categories_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can do the similar thing with state, teacher_prefix and project_grade_category also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF of preprocessed_essays for X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10330\n"
     ]
    }
   ],
   "source": [
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_model.fit(X_train_preprocessed_essays)\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "X_train_dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n",
    "X_train_tfidf_words = set(tfidf_model.get_feature_names())\n",
    "print (len(X_train_tfidf_words))\n",
    "X_train_tfidf = tfidf_model.transform(X_train_preprocessed_essays)\n",
    "X_test_tfidf = tfidf_model.transform(X_test_preprocessed_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X_train TFIDF of preprocessed_essays for X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7607\n"
     ]
    }
   ],
   "source": [
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_model.fit(X_test_preprocessed_essays)\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "X_test_dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n",
    "X_test_tfidf_words = set(tfidf_model.get_feature_names())\n",
    "print (len(X_test_tfidf_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Vectorizing Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
    "# make sure you have the glove_vectors file\n",
    "with open('C:\\\\VipinML\\\\InputData\\\\glove_vectors', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words =  set(model.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of preprocessed_essays for X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1340/1340 [00:00<00:00, 3735.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_train_avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_train_preprocessed_essays): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    X_train_avg_w2v_vectors.append(vector)\n",
    "\n",
    "print(len(X_train_avg_w2v_vectors))\n",
    "print(len(X_train_avg_w2v_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:00<00:00, 3308.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_test_avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_test_preprocessed_essays): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    X_test_avg_w2v_vectors.append(vector)\n",
    "\n",
    "print(len(X_test_avg_w2v_vectors))\n",
    "print(len(X_test_avg_w2v_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TFIDF-W2W Vecorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1340/1340 [00:02<00:00, 502.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_train_tfidf_w2v_vectors_pessays = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_train_preprocessed_essays): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if (word in glove_words) and (word in X_train_tfidf_words):\n",
    "            vec = model[word] # getting the vector for each word\n",
    "            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "            tf_idf = X_train_dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "            tf_idf_weight += tf_idf\n",
    "    if tf_idf_weight != 0:\n",
    "        vector /= tf_idf_weight\n",
    "    X_train_tfidf_w2v_vectors_pessays.append(vector)\n",
    "\n",
    "print(len(X_train_tfidf_w2v_vectors_pessays))\n",
    "print(len(X_train_tfidf_w2v_vectors_pessays[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:01<00:00, 496.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_test_tfidf_w2v_vectors_pessays = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_test_preprocessed_essays): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if (word in glove_words) and (word in X_test_tfidf_words):\n",
    "            vec = model[word] # getting the vector for each word\n",
    "            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "            tf_idf = X_test_dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "            tf_idf_weight += tf_idf\n",
    "    if tf_idf_weight != 0:\n",
    "        vector /= tf_idf_weight\n",
    "    X_test_tfidf_w2v_vectors_pessays.append(vector)\n",
    "\n",
    "print(len(X_test_tfidf_w2v_vectors_pessays))\n",
    "print(len(X_test_tfidf_w2v_vectors_pessays[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:00<00:00, 82700.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_test_tfidf_w2v_vectors_ptitle = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_test['project_title']): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if (word in glove_words) and (word in X_test_tfidf_words):\n",
    "            vec = model[word] # getting the vector for each word\n",
    "            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "            tf_idf = X_test_dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "            tf_idf_weight += tf_idf\n",
    "    if tf_idf_weight != 0:\n",
    "        vector /= tf_idf_weight\n",
    "    X_test_tfidf_w2v_vectors_ptitle.append(vector)\n",
    "\n",
    "print(len(X_test_tfidf_w2v_vectors_ptitle))\n",
    "print(len(X_test_tfidf_w2v_vectors_ptitle[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1340/1340 [00:00<00:00, 89573.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "\n",
    "X_train_tfidf_w2v_vectors_ptitle = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_train['project_title']): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if (word in glove_words) and (word in X_train_tfidf_words):\n",
    "            vec = model[word] # getting the vector for each word\n",
    "            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "            tf_idf = X_train_dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "            tf_idf_weight += tf_idf\n",
    "    if tf_idf_weight != 0:\n",
    "        vector /= tf_idf_weight\n",
    "    X_train_tfidf_w2v_vectors_ptitle.append(vector)\n",
    "\n",
    "print(len(X_train_tfidf_w2v_vectors_ptitle))\n",
    "print(len(X_train_tfidf_w2v_vectors_ptitle[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of teacher_prefix  for X_train,X_test, X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after one hot encodig  (1340, 30)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one hot encoded features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=10,ngram_range=(1,4), vocabulary=list(sorted_teacher_dict.keys()),max_features=5000, lowercase=False, binary=True)\n",
    "X_train_teacher_prefix_data = X_train['teacher_prefix']\n",
    "\n",
    "X_train_teacher_prefix_data.fillna(\"Mrs.\", inplace = True) \n",
    "\n",
    "teacher_prefix_notnull = X_train_teacher_prefix_data[pd.notnull(X_train_teacher_prefix_data)]\n",
    "\n",
    "vectorizer.fit(teacher_prefix_notnull.values)\n",
    "\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "#print(teacher_prefix_notnull.values)\n",
    "\n",
    "X_train_teacher_prefix_one_hot = vectorizer.fit_transform(teacher_prefix_notnull.values)\n",
    "print(\"Shape of matrix after one hot encodig \",X_train_teacher_prefix_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after one hot encodig  (660, 30)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one hot encoded features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer1 = CountVectorizer(min_df=10,ngram_range=(1,4), max_features=5000,vocabulary=list(sorted_teacher_dict.keys()), lowercase=False, binary=True)\n",
    "X_test_teacher_prefix_data = X_test['teacher_prefix']\n",
    "X_test_teacher_prefix_data.fillna(\"Mrs.\", inplace = True) \n",
    "teacher_prefix_notnull = X_test_teacher_prefix_data[pd.notnull(X_test_teacher_prefix_data)]\n",
    "vectorizer.fit(teacher_prefix_notnull.values)\n",
    "X_test_teacher_prefix_one_hot = vectorizer1.transform(teacher_prefix_notnull.values)\n",
    "print(\"Shape of matrix after one hot encodig \",X_test_teacher_prefix_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of price for X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>school_state</th>\n",
       "      <th>Date</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>...</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>project_title_count</th>\n",
       "      <th>essay_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>151289</td>\n",
       "      <td>p212738</td>\n",
       "      <td>51f721456445bb57d5851935fb80ab9f</td>\n",
       "      <td>TX</td>\n",
       "      <td>2016-09-09 13:12:54</td>\n",
       "      <td>Grades_3_5</td>\n",
       "      <td>Math Manipulatives for Magnificent Minds</td>\n",
       "      <td>I have a very diverse population of students. ...</td>\n",
       "      <td>The supplies I am asking for I already have pl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>My students need manipulatives to help make ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>Math_Science</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>I have a very diverse population of students. ...</td>\n",
       "      <td>103.01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       id                        teacher_id school_state  \\\n",
       "848      151289  p212738  51f721456445bb57d5851935fb80ab9f           TX   \n",
       "\n",
       "                   Date project_grade_category  \\\n",
       "848 2016-09-09 13:12:54             Grades_3_5   \n",
       "\n",
       "                                project_title  \\\n",
       "848  Math Manipulatives for Magnificent Minds   \n",
       "\n",
       "                                       project_essay_1  \\\n",
       "848  I have a very diverse population of students. ...   \n",
       "\n",
       "                                       project_essay_2 project_essay_3  ...  \\\n",
       "848  The supplies I am asking for I already have pl...             NaN  ...   \n",
       "\n",
       "                              project_resource_summary  \\\n",
       "848  My students need manipulatives to help make ma...   \n",
       "\n",
       "    teacher_number_of_previously_posted_projects  clean_categories  \\\n",
       "848                                            0      Math_Science   \n",
       "\n",
       "    clean_subcategories teacher_prefix  \\\n",
       "848         Mathematics    Mathematics   \n",
       "\n",
       "                                                 essay   price  quantity  \\\n",
       "848  I have a very diverse population of students. ...  103.01       4.0   \n",
       "\n",
       "     project_title_count  essay_count  \n",
       "848                    5          220  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(1)\n",
    "X_test.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "<class 'numpy.ndarray'>\n",
      "(1340, 1) (1340,)\n",
      "(660, 1) (660,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "# normalizer.fit(X_train['price'].values)\n",
    "# this will rise an error Expected 2D array, got 1D array instead: \n",
    "# array=[105.22 215.96  96.01 ... 368.98  80.53 709.67].\n",
    "# Reshape your data either using \n",
    "# array.reshape(-1, 1) if your data has a single feature \n",
    "# array.reshape(1, -1)  if it contains a single sample.\n",
    "\n",
    "#normalizer.fit(X_train['price'].values.reshape(-1,1))\n",
    "\n",
    "X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(1,-1))\n",
    "X_test_price_norm = normalizer.transform(X_test['price'].values.reshape(1,-1))\n",
    "X_train_price_norm= X_train_price_norm.reshape(-1,1)\n",
    "X_test_price_norm=X_test_price_norm.reshape(-1,1)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print (type(X_train_price_norm))\n",
    "print(X_train_price_norm.shape, y_train.shape)\n",
    "print(X_test_price_norm.shape, y_test.shape)\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Normalization of Project Title Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(1340, 1) (1340,)\n",
      "(660, 1) (660,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "#print (X_train['project_title_count'])\n",
    "\n",
    "X_train_project_title_count_norm = normalizer.fit_transform(X_train['project_title_count'].values.reshape(1,-1))\n",
    "X_test_project_title_count_norm = normalizer.transform(X_test['project_title_count'].values.reshape(1,-1))\n",
    "\n",
    "X_train_project_title_count_norm= X_train_project_title_count_norm.reshape(-1,1)\n",
    "X_test_project_title_count_norm=X_test_project_title_count_norm.reshape(-1,1)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_project_title_count_norm.shape, y_train.shape)\n",
    "print(X_test_project_title_count_norm.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Normalization of essay count words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(1340, 1) (1340,)\n",
      "(660, 1) (660,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "#print (X_train['project_title_count'])\n",
    "\n",
    "X_train_essay_count_norm = normalizer.fit_transform(X_train['essay_count'].values.reshape(1,-1))\n",
    "X_test_essay_count_norm = normalizer.transform(X_test['essay_count'].values.reshape(1,-1))\n",
    "\n",
    "X_train_essay_count_norm= X_train_essay_count_norm.reshape(-1,1)\n",
    "X_test_essay_count_norm=X_test_essay_count_norm.reshape(-1,1)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_essay_count_norm.shape, y_train.shape)\n",
    "print(X_test_essay_count_norm.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(1340, 1) (1340,)\n",
      "(660, 1) (660,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "#print (X_train['project_title_count'])\n",
    "\n",
    "X_train_quantity_norm = normalizer.fit_transform(X_train['quantity'].values.reshape(1,-1))\n",
    "X_test_quantity_norm = normalizer.transform(X_test['quantity'].values.reshape(1,-1))\n",
    "\n",
    "X_train_quantity_norm= X_train_quantity_norm.reshape(-1,1)\n",
    "X_test_quantity_norm=X_test_quantity_norm.reshape(-1,1)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_quantity_norm.shape, y_train.shape)\n",
    "print(X_test_quantity_norm.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(1340, 1) (1340,)\n",
      "(660, 1) (660,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "X_train_teacher_number_of_previously_posted_projects_norm = normalizer.fit_transform(X_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "X_test_teacher_number_of_previously_posted_projects_norm = normalizer.transform(X_test['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "\n",
    "X_train_teacher_number_of_previously_posted_projects_norm= X_train_teacher_number_of_previously_posted_projects_norm.reshape(-1,1)\n",
    "X_test_teacher_number_of_previously_posted_projects_norm=X_test_teacher_number_of_previously_posted_projects_norm.reshape(-1,1)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_teacher_number_of_previously_posted_projects_norm.shape, y_train.shape)\n",
    "print(X_test_teacher_number_of_previously_posted_projects_norm.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words of preprocessed_essays for X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix X_train_text_bow after one hot encodig  (1340, 4166)\n",
      "Shape of matrix X_test_text_bow after one hot encodig  (660, 4166)\n"
     ]
    }
   ],
   "source": [
    "# We are considering only the words which appeared in at least 10 documents(rows or projects).\n",
    "vectorizer = CountVectorizer(min_df=10,ngram_range=(1,4),  max_features=5000)\n",
    "X_train_text_bow = vectorizer.fit_transform(X_train_preprocessed_essays)\n",
    "X_test_text_bow = vectorizer.transform(X_test_preprocessed_essays)\n",
    "\n",
    "print(\"Shape of matrix X_train_text_bow after one hot encodig \",X_train_text_bow.shape)\n",
    "print(\"Shape of matrix X_test_text_bow after one hot encodig \",X_test_text_bow.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words of project_title for X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix X_train_project_title_bow after one hot encodig  (1340, 134)\n",
      "Shape of matrix X_test_project_title_bow after one hot encodig  (660, 134)\n"
     ]
    }
   ],
   "source": [
    "# PROJECT_TITLE BOW\n",
    "# We are considering only the words which appeared in at least 10 documents(rows or projects). \n",
    "vectorizer = CountVectorizer(min_df=10,ngram_range=(1,4), max_features=5000)\n",
    "X_train_project_title_bow = vectorizer.fit_transform(X_train['project_title'])\n",
    "X_test_project_title_bow = vectorizer.transform(X_test['project_title'])\n",
    "\n",
    "print(\"Shape of matrix X_train_project_title_bow after one hot encodig \",X_train_project_title_bow .shape)\n",
    "print(\"Shape of matrix X_test_project_title_bow after one hot encodig \",X_test_project_title_bow .shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF of preprocessed_essays for X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix X_train_text_tfidf after one hot encodig  (1340, 10330)\n",
      "Shape of matrix X_test_text_tfidf after one hot encodig  (660, 10330)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_text_tfidf = vectorizer.fit_transform(X_train_preprocessed_essays)\n",
    "X_test_text_tfidf = vectorizer.transform(X_test_preprocessed_essays)\n",
    "\n",
    "print(\"Shape of matrix X_train_text_tfidf after one hot encodig \",X_train_text_tfidf.shape)\n",
    "print(\"Shape of matrix X_test_text_tfidf after one hot encodig \",X_test_text_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF of Project Title for X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix  X_train_project_title_tfidf after one hot encodig  (1340, 110)\n",
      "Shape of matrix  X_test_project_title_tfidf after one hot encodig  (660, 110)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=10)\n",
    "X_train_project_title_tfidf = vectorizer.fit_transform((X_train['project_title']))\n",
    "X_test_project_title_tfidf = vectorizer.transform((X_test['project_title']))\n",
    "\n",
    "print(\"Shape of matrix  X_train_project_title_tfidf after one hot encodig \",X_train_project_title_tfidf.shape)\n",
    "print(\"Shape of matrix  X_test_project_title_tfidf after one hot encodig \",X_test_project_title_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF AVG W2V for Project Title for X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1340/1340 [00:00<00:00, 111972.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_train_project_title_avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_train['project_title']): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    X_train_project_title_avg_w2v_vectors.append(vector)\n",
    "\n",
    "print(len(X_train_project_title_avg_w2v_vectors))\n",
    "print(len(X_train_project_title_avg_w2v_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:00<00:00, 109576.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_test_project_title_avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_test['project_title']): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    X_test_project_title_avg_w2v_vectors.append(vector)\n",
    "\n",
    "print(len(X_test_project_title_avg_w2v_vectors))\n",
    "print(len(X_test_project_title_avg_w2v_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340, 61) (1340,)\n",
      "(660, 61) (660,)\n",
      "====================================================================================================\n",
      "(1340, 171) (1340,)\n",
      "(660, 171) (660,)\n",
      "====================================================================================================\n",
      "(1340, 125) (1340,)\n",
      "(660, 125) (660,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# merge two sparse matrices: https://stackoverflow.com/a/19710648/4084039\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "X_tr = hstack((X_train_price_norm,X_train_sub_categories_one_hot,X_train_teacher_prefix_one_hot)).tocsr()\n",
    "X_te = hstack((X_test_price_norm,X_test_sub_categories_one_hot,X_test_teacher_prefix_one_hot)).tocsr()\n",
    "\n",
    "#print (X_train_price_norm)\n",
    "X_tr_bow = hstack((X_train_price_norm,X_train_sub_categories_one_hot,X_train_teacher_prefix_one_hot,\\\n",
    "                  X_train_project_title_bow )).tocsr()\n",
    "X_te_bow = hstack((X_test_price_norm,X_test_sub_categories_one_hot,X_test_teacher_prefix_one_hot,\\\n",
    "                   X_test_project_title_bow)).tocsr()\n",
    "\n",
    "#X_tr_bow = hstack((X_train_price_norm,X_train_sub_categories_one_hot,X_train_teacher_prefix_one_hot,\\\n",
    "                #   X_train_text_bow,X_train_project_title_bow)).tocsr()\n",
    "#X_te_bow = hstack((X_test_price_norm,X_test_sub_categories_one_hot,X_test_teacher_prefix_one_hot,\\\n",
    "                  # X_test_text_bow,X_test_project_title_bow)).tocsr()\n",
    "\n",
    "X_tr_tfidf = hstack((X_train_sub_categories_one_hot,X_train_teacher_prefix_one_hot,X_train_price_norm,\\\n",
    "                     X_train_project_title_tfidf)).tocsr()\n",
    "X_te_tfidf = hstack((X_test_sub_categories_one_hot,X_test_teacher_prefix_one_hot,X_test_price_norm,\\\n",
    "                     X_test_project_title_tfidf)).tocsr()\n",
    "\n",
    "\n",
    "#X_tr_tfidf = hstack((X_train_sub_categories_one_hot,X_train_teacher_prefix_one_hot,X_train_price_norm,\\\n",
    "#                     X_train_project_title_tfidf,X_train_tfidf)).tocsr()\n",
    "#X_te_tfidf = hstack((X_test_sub_categories_one_hot,X_test_teacher_prefix_one_hot,X_test_price_norm,\\\n",
    "#                     X_test_project_title_tfidf,X_test_tfidf)).tocsr()\n",
    "\n",
    "X_tr_tfidf_w2v = hstack((X_train_sub_categories_one_hot,X_train_teacher_prefix_one_hot,X_train_price_norm,X_train_project_title_avg_w2v_vectors)).tocsr()\n",
    "X_te_tfidf_w2v = hstack((X_test_sub_categories_one_hot,X_test_teacher_prefix_one_hot,X_test_price_norm,X_test_project_title_avg_w2v_vectors)).tocsr()\n",
    "\n",
    "\n",
    "X_tr_avg_w2v = hstack((X_train_sub_categories_one_hot,X_train_teacher_prefix_one_hot,X_train_price_norm,X_train_project_title_avg_w2v_vectors)).tocsr()\n",
    "X_te_avg_w2v = hstack((X_test_sub_categories_one_hot,X_test_teacher_prefix_one_hot,X_test_price_norm,X_test_project_title_avg_w2v_vectors)).tocsr()\n",
    "\n",
    "#set 5\n",
    "\n",
    "X_te_set5 =hstack((X_test_school_state_one_hot,X_test_categories_one_hot,X_test_sub_categories_one_hot,\\\n",
    "                  X_test_teacher_prefix_one_hot,X_test_quantity_norm,X_test_price_norm,X_test_project_grade_category_one_hot,\\\n",
    "                  X_test_teacher_number_of_previously_posted_projects_norm,X_test_price_norm,\\\n",
    "                  X_test_project_title_count_norm,X_test_essay_count_norm))\n",
    "\n",
    "X_tr_set5 =hstack((X_train_school_state_one_hot,X_train_categories_one_hot, X_train_sub_categories_one_hot,\\\n",
    "                  X_train_teacher_prefix_one_hot,X_train_quantity_norm,X_train_price_norm,X_train_project_grade_category_one_hot,\\\n",
    "                  X_train_teacher_number_of_previously_posted_projects_norm,X_train_price_norm,X_train_project_title_count_norm,\\\n",
    "                  X_train_essay_count_norm))\n",
    "\n",
    "X_te_tfidf_avg_w2v = hstack((X_test_school_state_one_hot,X_test_categories_one_hot,X_test_sub_categories_one_hot,\\\n",
    "                  X_test_tfidf_w2v_vectors_pessays,X_test_tfidf_w2v_vectors_ptitle))\n",
    "\n",
    "X_tr_tfidf_avg_w2v = hstack((X_train_school_state_one_hot,X_train_categories_one_hot, X_train_sub_categories_one_hot,\\\n",
    "                  X_train_tfidf_w2v_vectors_pessays,X_train_tfidf_w2v_vectors_ptitle))\n",
    "\n",
    "#print(\"Final Data matrix\")\n",
    "print(X_tr.shape, y_train.shape)\n",
    "print(X_te.shape, y_test.shape)\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(X_tr_tfidf.shape, y_train.shape)\n",
    "print(X_te_tfidf.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(X_tr_set5.shape,y_train.shape )\n",
    "print(X_te_set5.shape,y_test.shape )\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: SVM\n",
    "\n",
    "<ol>\n",
    "    <li><strong>[Task-1] SVM (either SGDClassifier with log loss, or LogisticRegression) on these feature sets</strong>\n",
    "        <ul>\n",
    "            <li><font color='red'>Set 1</font>: categorical, numerical features + project_title(BOW) + preprocessed_eassay (`BOW with bi-grams` with `min_df=10` and `max_features=5000`)</li>\n",
    "            <li><font color='red'>Set 2</font>: categorical, numerical features + project_title(TFIDF)+  preprocessed_eassay (`TFIDF with bi-grams` with `min_df=10` and `max_features=5000`)</li>\n",
    "            <li><font color='red'>Set 3</font>: categorical, numerical features + project_title(AVG W2V)+  preprocessed_eassay (AVG W2V)</li>\n",
    "            <li><font color='red'>Set 4</font>: categorical, numerical features + project_title(TFIDF W2V)+  preprocessed_essay (TFIDF W2V)</li>        </ul>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li><strong>Hyper paramter tuning (find best hyper parameters corresponding the algorithm that you choose)</strong>\n",
    "        <ul>\n",
    "    <li>Find the best hyper parameter which will give the maximum <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/receiver-operating-characteristic-curve-roc-curve-and-auc-1/'>AUC</a> value</li>\n",
    "    <li>Find the best hyper paramter using k-fold cross validation or simple cross validation data</li>\n",
    "    <li>Use gridsearch cv or randomsearch cv or you can also write your own for loops to do this task of hyperparameter tuning</li>          \n",
    "        </ul>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li><strong>Representation of results</strong>\n",
    "        <ul>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure.\n",
    "    <img src='train_cv_auc.JPG' width=300px></li>\n",
    "    <li>Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n",
    "    <img src='train_test_auc.JPG' width=300px></li>\n",
    "    <li>Along with plotting ROC curve, you need to print the <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/confusion-matrix-tpr-fpr-fnr-tnr-1/'>confusion matrix</a> with predicted and original labels of test data points. Please visualize your confusion matrices using <a href='https://seaborn.pydata.org/generated/seaborn.heatmap.html'>seaborn heatmaps.\n",
    "    <img src='confusion_matrix.png' width=300px></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li><strong>[Task-2] Apply Logistic Regression on the below feature set <font color='red'> Set 5 </font> by finding the best hyper parameter as suggested in step 2 and step 3.</strong>\n",
    "    <li> Consider these set of features <font color='red'> Set 5 :</font>\n",
    "            <ul>\n",
    "                <li><strong>school_state</strong> : categorical data</li>\n",
    "                <li><strong>clean_categories</strong> : categorical data</li>\n",
    "                <li><strong>clean_subcategories</strong> : categorical data</li>\n",
    "                <li><strong>project_grade_category</strong> :categorical data</li>\n",
    "                <li><strong>teacher_prefix</strong> : categorical data</li>\n",
    "                <li><strong>quantity</strong> : numerical data</li>\n",
    "                <li><strong>teacher_number_of_previously_posted_projects</strong> : numerical data</li>\n",
    "                <li><strong>price</strong> : numerical data</li>\n",
    "                <li><strong>sentiment score's of each of the essay</strong> : numerical data</li>\n",
    "                <li><strong>number of words in the title</strong> : numerical data</li>\n",
    "                <li><strong>number of words in the combine essays</strong> : numerical data</li>\n",
    "            </ul>\n",
    "        And apply the Logistic regression on these features by finding the best hyper paramter as suggested in step 2 and step 3 <br>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li><strong>Conclusion</strong>\n",
    "        <ul>\n",
    "    <li>You need to summarize the results at the end of the notebook, summarize it in the table format. To print out a table please refer to this prettytable library<a href='http://zetcode.com/python/prettytable/'>  link</a> \n",
    "        <img src='summary.JPG' width=400px>\n",
    "    </li>\n",
    "        </ul>\n",
    "</ol>\n",
    "\n",
    "<h1>2. SVM </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.4 Appling SVM on different kind of featurization as mentioned in the instructions</h2>\n",
    "\n",
    "<br>Apply SVM on different kind of featurization as mentioned in the instructions\n",
    "<br> For Every model that you work on make sure you do the step 2 and step 3 of instrucations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(clf, data):\n",
    "    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "    # not the predicted outputs\n",
    "    y_data_pred = []\n",
    "    tr_loop = data.shape[0] - data.shape[0]%1000\n",
    "    # consider you X_tr shape is 49041, then your tr_loop will be 49041 - 49041%1000 = 49000\n",
    "    # in this for loop we will iterate unti the last 1000 multiplier\n",
    "    for i in range(0, tr_loop, 1000):\n",
    "        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])\n",
    "    # we will be predicting for the last data points\n",
    "    if data.shape[0]%1000 !=0:\n",
    "        y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n",
    "    \n",
    "    return y_data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are writing our own function for predict, with defined thresould\n",
    "# we will pick a threshold that will give the least fpr\n",
    "def find_best_threshold(threshould, fpr, tpr):\n",
    "    t = threshould[np.argmax(tpr*(1-fpr))]\n",
    "    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n",
    "    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n",
    "    return t\n",
    "\n",
    "def predict_with_best_t(proba, threshould):\n",
    "    predictions = []\n",
    "    for i in proba:\n",
    "        if i>=threshould:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_validation(X_train,y_train,X_test,y_test):\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn import svm\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \"\"\"\n",
    "    y_true : array, shape = [n_samples] or [n_samples, n_classes]\n",
    "    True binary labels or binary label indicators.\n",
    "\n",
    "    y_score : array, shape = [n_samples] or [n_samples, n_classes]\n",
    "    Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of\n",
    "    decisions (as returned by “decision_function” on some classifiers). \n",
    "    For binary y_true, y_score is supposed to be the score of the class with greater label.\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    #Using GridSearchCV\n",
    "    model = GridSearchCV(svm.SVC(kernel='rbf', probability=True), param_grid, cv=3)\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"model.best_estimator_ = %s\" % model.best_estimator_)\n",
    "    print(\"model.score = %s\" % model.score(X_test, y_test))\n",
    "\n",
    "    y_train_pred = model.predict_proba(X_train)    \n",
    "    y_test_pred = model.predict_proba(X_test)\n",
    "\n",
    "   # calculate accuracy of class predictions\n",
    "    from sklearn import metrics\n",
    "   # print (metrics.accuracy_score(y_test, y_test_pred[:, 1]))\n",
    "\n",
    "    #the ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.\n",
    "    train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred[:, 1])\n",
    "    test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred[:, 1])\n",
    "    trainAUC= float(\"{0:.2f}\".format(auc(train_fpr, train_tpr)))\n",
    "    testAUC = float(\"{0:.2f}\".format(auc(test_fpr, test_tpr)))\n",
    "\n",
    "    \n",
    "    plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(trainAUC))\n",
    "    plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(testAUC))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(\"AUC PLOT\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "     \n",
    "    print(\"=\"*100)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n",
    "    print(\"Train confusion matrix\")\n",
    "    print(confusion_matrix(y_train, predict_with_best_t(y_train_pred[:, 1], best_t)))\n",
    "    print(\"Test confusion matrix\")\n",
    "    print(confusion_matrix(y_test, predict_with_best_t(y_test_pred[:, 1], best_t)))\n",
    "    \n",
    "    return [trainAUC,testAUC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_for_Best_Hyper_Parameter(X_train,y_train,X_test,y_test, HyperParameter):\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    model = SVC(gamma='auto', probability=True )   \n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict_proba(X_train)    \n",
    "    y_test_pred = model.predict_proba(X_test)\n",
    "   \n",
    "    #The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.\n",
    "    train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred[:, 1])\n",
    "    test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred[:, 1])\n",
    "\n",
    "    import seaborn as sns\n",
    "   \n",
    "    print(\"=\"*100)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n",
    "    print(\"Train confusion matrix\")\n",
    "    train_matrix = confusion_matrix(y_train, predict_with_best_t(y_train_pred[:, 1], best_t))\n",
    "    print(train_matrix)\n",
    "                                   \n",
    "    print(\"Test confusion matrix\")\n",
    "    test_matrix = confusion_matrix(y_test, predict_with_best_t(y_test_pred[:, 1], best_t))\n",
    "    print(test_matrix)\n",
    "   \n",
    "    trainAUC= float(\"{0:.2f}\".format(auc(train_fpr, train_tpr)))\n",
    "    testAUC = float(\"{0:.2f}\".format(auc(test_fpr, test_tpr)))\n",
    "\n",
    "    \n",
    "    plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(trainAUC))\n",
    "    plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(testAUC))\n",
    "    \n",
    "    # plot AUC curve. AUC curve should show best accuracy rate, since best aplha is used in the logic.\n",
    "    plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(trainAUC))\n",
    "    plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(testAUC))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(\"AUC PLOT\")\n",
    "    plt.grid()\n",
    "    \n",
    "     # Confusiomatrix heatmap.\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,sharex=True, sharey=True)\n",
    "  #  plt.figure(figsize=(30, 60))\n",
    "    g1=sns.heatmap(test_matrix, annot=True,fmt='',cbar=True, linewidths =0.3, ax=ax1)\n",
    "    g1.set_xlabel(\"Test confusion matrix\")\n",
    "    g1.axes.get_xaxis().set_visible(True)\n",
    "    g1.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "    g2=sns.heatmap(train_matrix,annot=True,fmt='',cbar=True,linewidths =0.3,ax=ax2)\n",
    "    g2.set_xlabel(\"Train confusion matrix\")\n",
    "    g2.axes.get_xaxis().set_visible(True)\n",
    "    g2.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "    plt.show()\n",
    "    return [trainAUC,testAUC]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the normalized data into training and test sets\n",
    "Logic below is simialr as covred in kanalysis_cross_validation(X,y), but here logic is for to  calculate confusion matrix, acuarcy ration for best K as we already foound best K after trying best accuracy for multiple K values.\n",
    "We can apply  K -fold CV to either the hyperparameter tuning, performance reporting, or both. The advantage of this approach is that the performance is less sensitive to unfortunate splits of data. In addition, it utilize data better since each example can be used for both training and validation/testing.\n",
    "\n",
    "Let's use  K -Fold CV to select the hyperparamter n_neighbors of the KNeighborsClassifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to speculate the performance of the model using ROC Curve?\n",
    "#### An excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability. In fact it means it is reciprocating the result. It is predicting 0s as 1s and 1s as 0s. And when AUC is 0.5, it means model has no class separation capacity whatsoever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the normalized data into training and test sets\n",
    "This step is required to prepare us for the fitting (i.e. training) the #model later. The “X” variable is a collection of all the features. The “y” variable is the target label which specifies the #classification of 1 or 0 based. Our goal will be to identify which category the new data point should fall into. Evaluate the predictions. Evaluate the Model by reviewing the classification report or confusion matrix. By reviewing these tables, we are able to evaluate how accurate our model is with new values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used for passsing various Hyper parameter in a loop,  and get the best parameter \n",
    "# that would give best accuracy. For  each Hyperparemer, predicted value and accuracy  is calculated. \n",
    "# best Hyperparameter  is reHyper  for best accuracy. this is like gridCVSearch but shown plots for various hymer parametr.\n",
    "# after best Hyperparam is returnbed, that is used  and again best AUC plot is drawn.\n",
    "# Items 4 in your query, first three items are covered here.\n",
    "\n",
    "\n",
    "def SVM_HyperParam_Analysis(X_train,y_train,X_test,y_test):\n",
    "    from sklearn import model_selection\n",
    "    from mlxtend.plotting import plot_decision_regions\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import math\n",
    "    # Import classification report and confusion matrix to evaluate predictions\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    train_auc = []\n",
    "    test_auc = []\n",
    "    #Hyperparams = [10**-6, 10**-5,10**-4, 10**-2,10**0, 10,20, 30,10**2, 10**3, 10**4]\n",
    "    Hyperparams = np.logspace(-4,4, 30)\n",
    "    #print (Hyperparams)\n",
    "    #Hyperparams= [10**x for x in range (-4,5)]\n",
    "    \n",
    "       \n",
    "    best_accuracy=0.0001\n",
    "    LogHyperparams =[]\n",
    "    for i in Hyperparams:\n",
    "        if (i >0  and math.log(i) >0):\n",
    "            HyperParameter = math.log(i)\n",
    "            LogHyperparams.append(HyperParameter)\n",
    "            model  = SVC(gamma='auto')\n",
    "\n",
    "            # fitting the model on crossvalidation train\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # predict the response on the crossvalidation train\n",
    "            y_train_pred = model.predict(X_train)  # predicting the value using cross validation data. \n",
    "\n",
    "            # predict the response on the crossvalidation test\n",
    "            y_test_pred = model.predict(X_test)  # predicting the value using cross validation data. \n",
    "\n",
    "\n",
    "            # evaluate CV accuracy\n",
    "            acc = accuracy_score(y_test, y_test_pred, normalize=True) * float(100)  # I get the accuracy score. \n",
    "            print (\"accuracy score = %s, Hyper Parameter  = %s \" , acc, HyperParameter)\n",
    "            if acc > best_accuracy:\n",
    "                best_accuracy =acc\n",
    "                HyperParam = HyperParameter\n",
    "                \n",
    "               \n",
    "           # print('\\n Test Accuracy for Hyper Parameter = %s is %s' % (HyperParameter, acc))\n",
    "           # print(\"=========================================\")\n",
    "\n",
    "            # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "            # not the predicted outputs        \n",
    "            train_auc.append(roc_auc_score(y_train,y_train_pred))\n",
    "            test_auc.append(roc_auc_score(y_test, y_test_pred))\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    print('\\n Test Accuracy for Hyper Parameter = %s is %s' % (HyperParam, best_accuracy))\n",
    "    \n",
    "    plt.plot(LogHyperparams, train_auc, label='Train AUC')\n",
    "    plt.plot(LogHyperparams, test_auc, label='Test AUC')\n",
    "\n",
    "    plt.scatter(LogHyperparams, train_auc, label='Train AUC points')\n",
    "    plt.scatter(LogHyperparams, test_auc, label='Test AUC points')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Log (Hyperparams: Hyperparameter)\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.title(\"AUC vrs Log(Hyperparameter)\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "       \n",
    "    return HyperParam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.0 Applying SVM  Set 1: categorical, numerical features + project_title(BOW) + preprocessed_essay (BOW),<font color='red'> SET 1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.best_estimator_ = SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "model.score = 0.85\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "the maximum value of tpr*(1-fpr) 0.25840018096713013 for threshold 0.867\n",
      "Train confusion matrix\n",
      "[[108  94]\n",
      " [588 550]]\n",
      "Test confusion matrix\n",
      "[[ 61  38]\n",
      " [361 200]]\n"
     ]
    }
   ],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "trainAUC, testAUC = SVM_validation (X_tr,y_train,X_te,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Applying SVM on BOW - Set 1: categorical, numerical features + project_title(BOW) + preprocessed_essay (BOW),<font color='red'> SET 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.best_estimator_ = SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "model.score = 0.85\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "the maximum value of tpr*(1-fpr) 0.3017931406497416 for threshold 0.867\n",
      "Train confusion matrix\n",
      "[[111  91]\n",
      " [513 625]]\n",
      "Test confusion matrix\n",
      "[[ 50  49]\n",
      " [250 311]]\n"
     ]
    }
   ],
   "source": [
    "trainAUC_bow, testAUC_bow = SVM_validation (X_tr_bow,y_train,X_te_bow,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Applying SVM  on TFIDF Set 2: categorical, numerical features + project_title(TFIDF)+ preprocessed_essay (TFIDF),<font color='red'> SET 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAUC_tfidf, testAUC_tfidf = SVM_validation (X_tr_tfidf,y_train,X_te_tfidf,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Applying SVM on TFIDF Avg W2V <font color='red'> SET 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAUC_avg_w2v, testAUC_avg_w2v = SVM_validation (X_tr_tfidf_avg_w2v,y_train,X_te_tfidf_avg_w2v,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Applying SVM on AVG W2V,<font color='red'> SET 3</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainAUC_avg_w2v, testAUC_avg_w2v = SVM_validation (X_tr_avg_w2v,y_train,X_te_avg_w2v,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Applying SVM on TFIDF W2V,<font color='red'> SET 4</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAUC_tfidf_w2v, testAUC_tfidf_w2v = SVM_validation(X_tr_tfidf_w2v,y_train,X_te_tfidf_w2v,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Applying SVM  on TFIDF Set 5: categorical, numerical features <font color='red'> SET 5</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "trainAUA_set, testAUC_set = SVM_validation (X_tr_set5,y_train,X_te_set5,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5  Feature selection for Best Hyper Parameter /font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.5.1 <font color='red'> Hyper Param-Analysis  <font color='blue'> categorical, numerical features + project_title(BOW) + preprocessed_essay (BOW),<font color='red'> SET 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_hyperparam= SVM_HyperParam_Analysis(X_tr_bow,y_train,X_te_bow,y_test)\n",
    "print (\"Hyper Param to apply is %s\" % bow_hyperparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 <font color='red'> Hyper Param -Analysis   <font color='blue'>  on TFIDF Set 2: categorical, numerical features + project_title(TFIDF)+ preprocessed_essay (TFIDF),<font color='red'> SET 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_hyperparam= SVM_HyperParam_Analysis(X_tr_tfidf,y_train,X_te_tfidf,y_test)\n",
    "print (\"Hyper Param to apply is %s\" % tfidf_hyperparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 <font color='red'> Hyper Parameter-Analysis     <font color='blue'>  on AVG W2V - categorical, numerical features + project_title(AVG W2V )+ preprocessed_essay (AVG W2V ),<font color='red'> SET 3</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgw2v_hyperparam= SVM_HyperParam_Analysis(X_tr_avg_w2v,y_train,X_te_avg_w2v,y_test)\n",
    "print (\"Hyper Param to apply is %s\" % avgw2v_hyperparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 <font color='red'> Hyper Parameter-Analysis     <font color='blue'>  on TFIDF W2V - categorical, numerical features + project_title(TFIDF W2V )+ preprocessed_essay (TFIDF W2V ),<font color='red'> SET 3</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfw2v_hyperparam= SVM_HyperParam_Analysis(X_tr_tfidf_w2v,y_train,X_te_tfidf_w2v,y_test)\n",
    "print (\"Hyper Param to apply is %s\" % tfidfw2v_hyperparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Param Analysis on TFIDF Set 5: categorical, numerical features + SET 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic Regression Analysis on Best Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set5_hyperparam = SVM_HyperParam_Analysis(X_tr_set5,y_train,X_te_set5,y_test)\n",
    "print (\"Hyper Param to apply is %s\" % set5_hyperparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfw2v_hyperparam = SVM_HyperParam_Analysis(X_tr_tfidf_w2v,y_train,X_te_tfidf_w2v,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParameter = set5_hyperparam\n",
    "trainAUC_analysis, testAUC_analysis = SVM_for_Best_Hyper_Parameter(X_tr_set5,y_train,X_te_set5,y_test,HyperParameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParameter =bow_hyperparam\n",
    "trainAUC_bow_analysis, testAUC_bow_analysis =SVM_for_Best_Hyper_Parameter(X_tr_bow,y_train,X_te_bow,y_test, HyperParameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParameter = avgw2v_hyperparam\n",
    "trainAUC_avg_w2v_analysis, testAUC_avg_w2v_analysis = SVM_for_Best_Hyper_Parameter(X_tr_avg_w2v,y_train,X_te_avg_w2v,y_test, HyperParameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParameter = tfidf_hyperparam\n",
    "trainAUC_tfidf_analysis, testAUC_tfidf_analysis = SVM_for_Best_Hyper_Parameter(X_tr_tfidf,y_train,X_te_tfidf,y_test,HyperParameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParameter = tfidfw2v_hyperparam\n",
    "trainAUC_tfidf_w2v_analysis, testAUC_tfidf_w2v_analysis = SVM_for_Best_Hyper_Parameter(X_tr_tfidf_w2v,y_train,X_te_tfidf_w2v,y_test,HyperParameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 <font color='Blue'> Pretty Table  SET 3</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw Pretty Table using GridCVSearch\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "    \n",
    "LRTable = PrettyTable()\n",
    "\n",
    "LRTable.field_names = [\"Model Type\", \"Train AUC\", \"Test AUC\"]\n",
    "LRTable.add_row([\"Regular\", trainAUC, testAUC])\n",
    "LRTable.add_row([\"BoW\", trainAUC_bow, testAUC_bow])\n",
    "LRTable.add_row([\"TFIDF\", trainAUC_tfidf, testAUC_tfidf])\n",
    "LRTable.add_row([\"TFIDF_Avg_W2V\", trainAUC_avg_w2v, testAUC_avg_w2v])\n",
    "LRTable.add_row([\"TFIDF_W2V\", trainAUC_tfidf_w2v, testAUC_tfidf_w2v])\n",
    "print (LRTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw Pretty Table using Best Analysis. Pretty table is drawn based on \n",
    "#best AUC is calcukated by passing varipus Hyperparameter in loop.\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "    \n",
    "LRTable = PrettyTable()\n",
    "\n",
    "LRTable.field_names = [\"Model Type\", \"Train AUC\", \"Test AUC\"]\n",
    "LRTable.add_row([\"Regular_Analyis\", trainAUC_analysis, testAUC_analysis])\n",
    "LRTable.add_row([\"BoW_Analysis\", trainAUC_bow_analysis, testAUC_bow_analysis])\n",
    "LRTable.add_row([\"TFIDF_Analysis\", trainAUC_tfidf, testAUC_tfidf])\n",
    "LRTable.add_row([\"TFIDF_Avg_W2V_Analysis\", trainAUC_avg_w2v_analysis, testAUC_avg_w2v_analysis])\n",
    "LRTable.add_row([\"TFIDF_W2V_Analysis\", trainAUC_tfidf_w2v_analysis, testAUC_tfidf_w2v_analysis])\n",
    "print (LRTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Conclusions</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a stastical method for analyzing a dataset in which there are one or more independent variables that determine the outcome. \n",
    "\n",
    "## Summary of above program as below:\n",
    "\n",
    "Lot of plots are drawn for different data set between train and test data. Test data is very near to train data.\n",
    "Please see pretty table for all comparasions.\n",
    "\n",
    "### Step 1: Import the necessary Libraries\n",
    "we will need to import libraries that allow for data analysis and data visualization to get acclimated to the dataset. We will be using pandas, numpy, matplotlib and seaborn to conduct this. Data Exploration libraries\n",
    "\n",
    "### Step 2: Read in the dataset.\n",
    "We will use the pandas .read_csv() method to read in the dataset. Then we will use the. head() method to observe the first few rows of the data, to understand the information better. In our case, the feature(column) headers tell us pretty little. This is fine because we are merely trying to gain insight via classifying new data points by referencing it’s neighboring elements.\n",
    "\n",
    "### Step 3: Standardize (normalize) the data scale to prep for Logistic regression.\n",
    "Because the distance between pairs of points plays a critical part on the classification, it is necessary to normalize the data This will generate an array of values. \n",
    "\n",
    "### Step 4: Split the normalized data into training and test sets.\n",
    "This step is required to prepare us for the fitting (i.e. training) the model later. The “X” variable is a collection of all the features. The “y” variable is the target label which specifies the classification of 1 or 0 based. Our goal will be to identify which category the new data point should fall into.\n",
    "\n",
    "\n",
    "### Step 5: Create and Train the Model.\n",
    "Here we create a Logistic Regression Object and use the .fit() method to train the model. Upon completion of the model we should receive confirmation that the training has been complete\n",
    "\n",
    "Please see functions as covered below, used in above program: def SVM_validation(X,y): def \n",
    "\n",
    "### Step 6: Make Predictions.\n",
    "Here we review where our model was accurate and where it misclassified elements.\n",
    "\n",
    "Please see functions as covered below, used in above program: def SVM_validation(X,y):\n",
    "\n",
    "### Step 7: Evaluate the predictions.\n",
    "\n",
    "Evaluate the Model by reviewing the classification report or confusion matrix. By reviewing these tables, we are able to evaluate how accurate our model is with new values.\n",
    "\n",
    "def SVM_validation(X,y):\n",
    "\n",
    "### Setp 8:Classification Report :\n",
    "This tells us our model was around 84% accurate… Print out classification report and confusion matrix\n",
    "\n",
    "I have covered various set to show confusion matrix.\n",
    "\n",
    "Please see section 2. covered various data sets and created confusion matrix.\n",
    "\n",
    "### Step 9: Evaluate alternative Hyper Parameter for better predictions.\n",
    "To simplify the process of evaluating multiple cases of Alpha values, we create a function to derive the error using the average where our predictions were not equal to the test values.\n",
    "\n",
    "Please see section 2. covered various data sets and created error accuracy reports.\n",
    "\n",
    "### Step 10: Adjust Hyper Parameter value per error rate evaluations \n",
    "This is just fine tuning our model to increase accuracy. We will need to retrain our model with the new Alpha.\n",
    "Please see section 3 in above program. we have created confusion matrix for optimal Alpha value for various data sets. As we can see for optimal Alpha, Accuracy is much higher - so prediction is much better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
