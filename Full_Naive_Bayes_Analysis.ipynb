{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes NB - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<p>\n",
    "DonorsChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. Right now, a large number of volunteers is needed to manually screen each submission before it's approved to be posted on the DonorsChoose.org website.\n",
    "</p>\n",
    "<p>\n",
    "    Next year, DonorsChoose.org expects to receive close to 500,000 project proposals. As a result, there are three main problems they need to solve:\n",
    "<ul>\n",
    "<li>\n",
    "    How to scale current manual processes and resources to screen 500,000 projects so that they can be posted as quickly and as efficiently as possible</li>F\n",
    "    <li>How to increase the consistency of project vetting across different volunteers to improve the experience for teachers</li>\n",
    "    <li>How to focus volunteer time on the applications that need the most assistance</li>\n",
    "    </ul>\n",
    "</p>    \n",
    "<p>\n",
    "The goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the Essay Data\n",
    "\n",
    "<ul>\n",
    "Prior to May 17, 2016, the prompts for the essays were as follows:\n",
    "<li>__project_essay_1:__ \"Introduce us to your classroom\"</li>\n",
    "<li>__project_essay_2:__ \"Tell us more about your students\"</li>\n",
    "<li>__project_essay_3:__ \"Describe how your students will use the materials you're requesting\"</li>\n",
    "<li>__project_essay_3:__ \"Close by sharing why your project will make a difference\"</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "Starting on May 17, 2016, the number of essays was reduced from 4 to 2, and the prompts for the first 2 essays were changed to the following:<br>\n",
    "<li>__project_essay_1:__ \"Describe your students: What makes your students special? Specific details about their background, your neighborhood, and your school are all helpful.\"</li>\n",
    "<li>__project_essay_2:__ \"About your project: How will these materials make a difference in your students' learning and improve their school lives?\"</li>\n",
    "<br>For all projects with project_submitted_datetime of 2016-05-17 and later, the values of project_essay_3 and project_essay_4 will be NaN.\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the necessary Libraries\n",
    "we will need to import libraries that allow for data analysis and data visualization to get acclimated to the dataset. We will be using pandas, numpy, matplotlib and seaborn to conduct this. Data Exploration libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "warnings.filterwarnings(\"ignore\",'detected Windows; aliasing chunkize to chunkize_serial')\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read in the dataset.\n",
    "We will use the pandas .read_csv() method to read in the dataset. Then we will use the. head() method to observe the first few rows of the data, to understand the information better. In our case, the feature(column) headers tell us pretty little. This is fine because we are merely trying to gain insight via classifying new data points by referencing it’s neighboring elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data = pd.read_csv(\"C:\\\\VipinML\\\\Assignment 2\\\\Assignments_DonorsChoose_2018\\\\train_data.csv\")\n",
    "resource_data = pd.read_csv(\"C:\\\\VipinML\\Assignment 2\\\\Assignments_DonorsChoose_2018\\\\resources.csv\")\n",
    "#Limit the data for testing purpose since processing takes few hours for full set..\n",
    "\n",
    "project_data = project_data.head(2000)\n",
    "resource_data = resource_data.head(2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in train data (2000, 17)\n",
      "--------------------------------------------------\n",
      "The attributes of data : ['Unnamed: 0' 'id' 'teacher_id' 'teacher_prefix' 'school_state'\n",
      " 'project_submitted_datetime' 'project_grade_category'\n",
      " 'project_subject_categories' 'project_subject_subcategories'\n",
      " 'project_title' 'project_essay_1' 'project_essay_2' 'project_essay_3'\n",
      " 'project_essay_4' 'project_resource_summary'\n",
      " 'teacher_number_of_previously_posted_projects' 'project_is_approved']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points in train data\", project_data.shape)\n",
    "print('-'*50)\n",
    "print(\"The attributes of data :\", project_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>Date</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>100660</td>\n",
       "      <td>p234804</td>\n",
       "      <td>cbc0e38f522143b86d372f8b43d4cff3</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>GA</td>\n",
       "      <td>2016-04-27 00:53:00</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Applied Learning</td>\n",
       "      <td>Early Development</td>\n",
       "      <td>Flexible Seating for Flexible Learning</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>I teach at a low-income (Title 1) school. Ever...</td>\n",
       "      <td>We need a classroom rug that we can use as a c...</td>\n",
       "      <td>Benjamin Franklin once said, \\\"Tell me and I f...</td>\n",
       "      <td>My students need flexible seating in the class...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       id                        teacher_id teacher_prefix  \\\n",
       "473      100660  p234804  cbc0e38f522143b86d372f8b43d4cff3           Mrs.   \n",
       "\n",
       "    school_state                Date project_grade_category  \\\n",
       "473           GA 2016-04-27 00:53:00          Grades PreK-2   \n",
       "\n",
       "    project_subject_categories project_subject_subcategories  \\\n",
       "473           Applied Learning             Early Development   \n",
       "\n",
       "                              project_title  \\\n",
       "473  Flexible Seating for Flexible Learning   \n",
       "\n",
       "                                       project_essay_1  \\\n",
       "473  I recently read an article about giving studen...   \n",
       "\n",
       "                                       project_essay_2  \\\n",
       "473  I teach at a low-income (Title 1) school. Ever...   \n",
       "\n",
       "                                       project_essay_3  \\\n",
       "473  We need a classroom rug that we can use as a c...   \n",
       "\n",
       "                                       project_essay_4  \\\n",
       "473  Benjamin Franklin once said, \\\"Tell me and I f...   \n",
       "\n",
       "                              project_resource_summary  \\\n",
       "473  My students need flexible seating in the class...   \n",
       "\n",
       "     teacher_number_of_previously_posted_projects  project_is_approved  \n",
       "473                                             2                    1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to replace elements in list python: https://stackoverflow.com/a/2582163/4084039\n",
    "cols = ['Date' if x=='project_submitted_datetime' else x for x in list(project_data.columns)]\n",
    "#sort dataframe based on time pandas python: https://stackoverflow.com/a/49702492/4084039\n",
    "project_data['Date'] = pd.to_datetime(project_data['project_submitted_datetime'])\n",
    "project_data.drop('project_submitted_datetime', axis=1, inplace=True)\n",
    "project_data.sort_values(by=['Date'], inplace=True)\n",
    "\n",
    "# how to reorder columns pandas python: https://stackoverflow.com/a/13148611/4084039\n",
    "project_data = project_data[cols]\n",
    "project_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect All Features into global List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of features to appned all the features for BoW, TFIDF and std catagories.\n",
    "features =[]\n",
    "features_tfidf= []\n",
    "features_std =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 preprocessing of `project_subject_categories`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesssing_cleanup(text):\n",
    "    # Combining all the above stundents \n",
    "   \n",
    "    from tqdm import tqdm\n",
    "    X_text = []\n",
    "    # tqdm is for printing the status bar\n",
    "    for sentance in tqdm(text):\n",
    "        print (sentance)\n",
    "        sent = decontracted(sentance)\n",
    "        sent = sent.replace('\\\\r', ' ')\n",
    "        sent = sent.replace('\\\\\"', ' ')\n",
    "        sent = sent.replace('\\\\n', ' ')\n",
    "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "        # https://gist.github.com/sebleier/554280\n",
    "        sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "        X_text.append(sent.lower().strip())\n",
    "       # print (X_train_preprocessed_essays)\n",
    "    return X_text\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "catogories = list(project_data['project_subject_categories'].values)\n",
    "# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n",
    "\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n",
    "# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n",
    "cat_list = []\n",
    "for i in catogories:\n",
    "    temp = \"\"\n",
    "    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n",
    "    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n",
    "        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n",
    "            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n",
    "        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n",
    "        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n",
    "        temp = temp.replace('&','_') # we are replacing the & value into \n",
    "    cat_list.append(temp.strip())\n",
    "    \n",
    "project_data['clean_categories'] = cat_list\n",
    "project_data.drop(['project_subject_categories'], axis=1, inplace=True)\n",
    "\n",
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "for word in project_data['clean_categories'].values:\n",
    "    my_counter.update(word.split())\n",
    "\n",
    "cat_dict = dict(my_counter)\n",
    "sorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 preprocessing of `project_subject_subcategories`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_catogories = list(project_data['project_subject_subcategories'].values)\n",
    "# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n",
    "\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n",
    "# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n",
    "\n",
    "sub_cat_list = []\n",
    "for i in sub_catogories:\n",
    "    temp = \"\"\n",
    "    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n",
    "    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n",
    "        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n",
    "            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n",
    "        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n",
    "        temp +=j.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n",
    "        temp = temp.replace('&','_')\n",
    "    sub_cat_list.append(temp.strip())\n",
    "\n",
    "project_data['clean_subcategories'] = sub_cat_list\n",
    "project_data.drop(['project_subject_subcategories'], axis=1, inplace=True)\n",
    "\n",
    "# count of all the words in corpus python: https://stackoverflow.com/a/22898595/4084039\n",
    "my_counter = Counter()\n",
    "for word in project_data['clean_subcategories'].values:\n",
    "    my_counter.update(word.split())\n",
    "    \n",
    "sub_cat_dict = dict(my_counter)\n",
    "sorted_sub_cat_dict = dict(sorted(sub_cat_dict.items(), key=lambda kv: kv[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_cat = list(project_data['teacher_prefix'].values)\n",
    "# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n",
    "\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n",
    "# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n",
    "cat_list = []\n",
    "for i in teacher_cat:\n",
    "    temp = \"\"\n",
    "    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n",
    "    j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n",
    "    temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n",
    "    temp = temp.replace('&','_') # we are replacing the & value into \n",
    "    cat_list.append(temp.strip())\n",
    "\n",
    "\n",
    "project_data.drop(['teacher_prefix'], axis=1, inplace=True)\n",
    "project_data['teacher_prefix'] = sub_cat_list\n",
    "    \n",
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "for word in project_data['teacher_prefix'].values:\n",
    "    my_counter.update(word.split())\n",
    "\n",
    "cat_dict = dict(my_counter)\n",
    "sorted_teacher_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two column text dataframe: \n",
    "project_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n",
    "                        project_data[\"project_essay_2\"].map(str) + \\\n",
    "                        project_data[\"project_essay_3\"].map(str) + \\\n",
    "                        project_data[\"project_essay_4\"].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>school_state</th>\n",
       "      <th>Date</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>100660</td>\n",
       "      <td>p234804</td>\n",
       "      <td>cbc0e38f522143b86d372f8b43d4cff3</td>\n",
       "      <td>GA</td>\n",
       "      <td>2016-04-27 00:53:00</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Flexible Seating for Flexible Learning</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>I teach at a low-income (Title 1) school. Ever...</td>\n",
       "      <td>We need a classroom rug that we can use as a c...</td>\n",
       "      <td>Benjamin Franklin once said, \\\"Tell me and I f...</td>\n",
       "      <td>My students need flexible seating in the class...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AppliedLearning</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       id                        teacher_id school_state  \\\n",
       "473      100660  p234804  cbc0e38f522143b86d372f8b43d4cff3           GA   \n",
       "\n",
       "                   Date project_grade_category  \\\n",
       "473 2016-04-27 00:53:00          Grades PreK-2   \n",
       "\n",
       "                              project_title  \\\n",
       "473  Flexible Seating for Flexible Learning   \n",
       "\n",
       "                                       project_essay_1  \\\n",
       "473  I recently read an article about giving studen...   \n",
       "\n",
       "                                       project_essay_2  \\\n",
       "473  I teach at a low-income (Title 1) school. Ever...   \n",
       "\n",
       "                                       project_essay_3  \\\n",
       "473  We need a classroom rug that we can use as a c...   \n",
       "\n",
       "                                       project_essay_4  \\\n",
       "473  Benjamin Franklin once said, \\\"Tell me and I f...   \n",
       "\n",
       "                              project_resource_summary  \\\n",
       "473  My students need flexible seating in the class...   \n",
       "\n",
       "     teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "473                                             2                    1   \n",
       "\n",
       "    clean_categories clean_subcategories    teacher_prefix  \\\n",
       "473  AppliedLearning    EarlyDevelopment  EarlyDevelopment   \n",
       "\n",
       "                                                 essay  \n",
       "473  I recently read an article about giving studen...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.4.2.3 Using Pretrained Models: TFIDF weighted W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s a teacher at a Title 1 school, many of my students receive free or reduced price lunch and have extremely limited resources.  Some face challenges daily that many adults never have to experience. M\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "sent = decontracted(project_data['essay'].values[500])\n",
    "print(sent[1:200])\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s a teacher at a Title 1 school, many of my students receive free or reduced price lunch and have extremely limited resources.  Some face challenges daily that many adults never have to experience. M\n",
      "s a teacher at a Title 1 school, many of my students receive free or reduced price lunch and have extremely limited resources.  Some face challenges daily that many adults never have to experience. M\n"
     ]
    }
   ],
   "source": [
    "# \\r \\n \\t remove from string python: http://texthandler.com/info/remove-line-breaks-python/\n",
    "sent = sent.replace('\\\\r', ' ')\n",
    "sent = sent.replace('\\\\\"', ' ')\n",
    "sent = sent.replace('\\\\n', ' ')\n",
    "print(sent[1:200])\n",
    "print(sent[1:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s a teacher at a Title 1 school many of my students receive free or reduced price lunch and have extremely limited resources Some face challenges daily that many adults never have to experience My st\n"
     ]
    }
   ],
   "source": [
    "#remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
    "sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "print(sent[1:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Merging price with project_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id   price  quantity\n",
      "1  p003483   81.28        40\n",
      "2  p006068  557.98        16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>school_state</th>\n",
       "      <th>Date</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100660</td>\n",
       "      <td>p234804</td>\n",
       "      <td>cbc0e38f522143b86d372f8b43d4cff3</td>\n",
       "      <td>GA</td>\n",
       "      <td>2016-04-27 00:53:00</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Flexible Seating for Flexible Learning</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>I teach at a low-income (Title 1) school. Ever...</td>\n",
       "      <td>We need a classroom rug that we can use as a c...</td>\n",
       "      <td>Benjamin Franklin once said, \\\"Tell me and I f...</td>\n",
       "      <td>My students need flexible seating in the class...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AppliedLearning</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                        teacher_id school_state  \\\n",
       "0      100660  p234804  cbc0e38f522143b86d372f8b43d4cff3           GA   \n",
       "\n",
       "                 Date project_grade_category  \\\n",
       "0 2016-04-27 00:53:00          Grades PreK-2   \n",
       "\n",
       "                            project_title  \\\n",
       "0  Flexible Seating for Flexible Learning   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  I recently read an article about giving studen...   \n",
       "\n",
       "                                     project_essay_2  \\\n",
       "0  I teach at a low-income (Title 1) school. Ever...   \n",
       "\n",
       "                                     project_essay_3  \\\n",
       "0  We need a classroom rug that we can use as a c...   \n",
       "\n",
       "                                     project_essay_4  \\\n",
       "0  Benjamin Franklin once said, \\\"Tell me and I f...   \n",
       "\n",
       "                            project_resource_summary  \\\n",
       "0  My students need flexible seating in the class...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                             2                    1   \n",
       "\n",
       "  clean_categories clean_subcategories    teacher_prefix  \\\n",
       "0  AppliedLearning    EarlyDevelopment  EarlyDevelopment   \n",
       "\n",
       "                                               essay  price  quantity  \n",
       "0  I recently read an article about giving studen...    NaN       NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\n",
    "project_data = pd.merge(project_data, price_data, on='id', how='left')\n",
    "print (price_data[1:3])\n",
    "project_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>school_state</th>\n",
       "      <th>Date</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100660</td>\n",
       "      <td>p234804</td>\n",
       "      <td>cbc0e38f522143b86d372f8b43d4cff3</td>\n",
       "      <td>GA</td>\n",
       "      <td>2016-04-27 00:53:00</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Flexible Seating for Flexible Learning</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>I teach at a low-income (Title 1) school. Ever...</td>\n",
       "      <td>We need a classroom rug that we can use as a c...</td>\n",
       "      <td>Benjamin Franklin once said, \\\"Tell me and I f...</td>\n",
       "      <td>My students need flexible seating in the class...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AppliedLearning</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>EarlyDevelopment</td>\n",
       "      <td>I recently read an article about giving studen...</td>\n",
       "      <td>103.01</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                        teacher_id school_state  \\\n",
       "0      100660  p234804  cbc0e38f522143b86d372f8b43d4cff3           GA   \n",
       "\n",
       "                 Date project_grade_category  \\\n",
       "0 2016-04-27 00:53:00          Grades PreK-2   \n",
       "\n",
       "                            project_title  \\\n",
       "0  Flexible Seating for Flexible Learning   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  I recently read an article about giving studen...   \n",
       "\n",
       "                                     project_essay_2  \\\n",
       "0  I teach at a low-income (Title 1) school. Ever...   \n",
       "\n",
       "                                     project_essay_3  \\\n",
       "0  We need a classroom rug that we can use as a c...   \n",
       "\n",
       "                                     project_essay_4  \\\n",
       "0  Benjamin Franklin once said, \\\"Tell me and I f...   \n",
       "\n",
       "                            project_resource_summary  \\\n",
       "0  My students need flexible seating in the class...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                             2                    1   \n",
       "\n",
       "  clean_categories clean_subcategories    teacher_prefix  \\\n",
       "0  AppliedLearning    EarlyDevelopment  EarlyDevelopment   \n",
       "\n",
       "                                               essay   price  quantity  \n",
       "0  I recently read an article about giving studen...  103.01       4.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert NaN value to mean of the column\n",
    "project_data.fillna(project_data.mean(), inplace=True)\n",
    "project_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into Train and cross validation(or test): Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = project_data['project_is_approved'].values\n",
    "X = project_data.drop(['project_is_approved'], axis=1)\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "catogories_essay = list(project_data['essay'].values)\n",
    "# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n",
    "\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n",
    "# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n",
    "cat_essay_list = []\n",
    "for i in catogories_essay:\n",
    "    temp = \"\"\n",
    "    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n",
    "    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n",
    "        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n",
    "            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n",
    "        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n",
    "        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n",
    "        temp = temp.replace('&','_') # we are replacing the & value into \n",
    "    cat_essay_list.append(temp.strip())\n",
    "    \n",
    "project_data['clean_essay'] = cat_essay_list\n",
    "\n",
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "for word in project_data['clean_essay'].values:\n",
    "    my_counter.update(word.split())\n",
    "\n",
    "cat_essay_dict = dict(my_counter)\n",
    "sorted_cat_essay_dict = dict(sorted(cat_essay_dict.items(), key=lambda kv: kv[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "catogories_title = list(project_data['project_title'].values)\n",
    "# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n",
    "# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n",
    "project_title_list = []\n",
    "for i in catogories_title:\n",
    "    temp = \"\"\n",
    "    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n",
    "    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n",
    "        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n",
    "            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n",
    "        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n",
    "        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n",
    "        temp = temp.replace('&','_') # we are replacing the & value into \n",
    "    project_title_list.append(temp.strip())\n",
    "\n",
    "#project_data.drop('project_title', axis=1, inplace=True)\n",
    "#project_data['project_title'] = project_title_list\n",
    "\n",
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "for word in project_data['project_title'].values:\n",
    "    my_counter.update(word.split())\n",
    "\n",
    "project_title_dict = dict(my_counter)\n",
    "sorted_project_title_dict = dict(sorted(project_title_dict.items(), key=lambda kv: kv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 897/897 [00:00<00:00, 910.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "from tqdm import tqdm\n",
    "X_train_preprocessed_essays = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(X_train['essay'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    X_train_preprocessed_essays.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:00<00:00, 1043.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "from tqdm import tqdm\n",
    "X_test_preprocessed_essays = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(X_test['essay'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    X_test_preprocessed_essays.append(sent.lower().strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 443/443 [00:00<00:00, 907.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "from tqdm import tqdm\n",
    "X_cv_preprocessed_essays = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(X_cv['essay'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "    X_cv_preprocessed_essays.append(sent.lower().strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Standardize (normalize) the data scale to prep for KNN algorithm.\n",
    "Because the distance between pairs of points plays a critical part on the classification, it is necessary to normalize the data This will generate an array of values. Again, KNN depends on the distance between each feature. Please see Section 1 for all normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Vectorizing Categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/handling-categorical-and-numerical-features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of clean_categories for X_train,X_test, X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Warmth', 'Care_Hunger', 'History_Civics', 'Music_Arts', 'AppliedLearning', 'SpecialNeeds', 'Health_Sports', 'Math_Science', 'Literacy_Language']\n",
      "Shape of matrix X_train_categories_one_hot  after one hot encodig  (897, 9)\n",
      "Shape of matrix X_test_categories_one_hot after one hot encodig  (660, 9)\n",
      "Shape of matrix X_cv_categories_one_hot after one hot encodig  (443, 9)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_clean_cat = CountVectorizer(min_df=10,ngram_range=(1,4), max_features=100, vocabulary=list(sorted_cat_dict.keys()), lowercase=False, binary=True)\n",
    "X_train_categories_one_hot = vectorizer_clean_cat.transform(X_train['clean_categories'].values)\n",
    "X_test_categories_one_hot = vectorizer_clean_cat.transform(X_test['clean_categories'].values)\n",
    "X_cv_categories_one_hot = vectorizer_clean_cat.transform(X_cv['clean_categories'].values)\n",
    "print(vectorizer_clean_cat.get_feature_names())\n",
    "print(\"Shape of matrix X_train_categories_one_hot  after one hot encodig \",X_train_categories_one_hot.shape)\n",
    "print(\"Shape of matrix X_test_categories_one_hot after one hot encodig \",X_test_categories_one_hot.shape)\n",
    "print(\"Shape of matrix X_cv_categories_one_hot after one hot encodig \",X_cv_categories_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of clean_subcategories for X_train,X_test, X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Economics', 'ForeignLanguages', 'FinancialLiteracy', 'Extracurricular', 'CommunityService', 'ParentInvolvement', 'Civics_Government', 'Warmth', 'Care_Hunger', 'NutritionEducation', 'SocialSciences', 'CharacterEducation', 'History_Geography', 'PerformingArts', 'TeamSports', 'College_CareerPrep', 'Other', 'Music', 'ESL', 'EarlyDevelopment', 'Health_LifeScience', 'Gym_Fitness', 'VisualArts', 'EnvironmentalScience', 'AppliedSciences', 'Health_Wellness', 'SpecialNeeds', 'Literature_Writing', 'Mathematics', 'Literacy']\n",
      "Shape of matrix X_train_sub_categories_one_hot  after one hot encodig  (897, 30)\n",
      "Shape of matrix X_test_sub_categories_one_hot after oneX_test_sub_categories_one_hot  hot encodig  (660, 30)\n",
      "Shape of matrixX_cv_sub_categories_one_hot after one hot encodig  (443, 30)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one \n",
    "vectorizer_sub_cat = CountVectorizer(min_df=10,ngram_range=(1,4), max_features=100,vocabulary=list(sorted_sub_cat_dict.keys()), lowercase=False, binary=True)\n",
    "X_train_sub_categories_one_hot = vectorizer_sub_cat.transform(X_train['clean_subcategories'].values)\n",
    "X_test_sub_categories_one_hot = vectorizer_sub_cat.transform(X_test['clean_subcategories'].values)\n",
    "X_cv_sub_categories_one_hot = vectorizer_sub_cat.transform(X_cv['clean_subcategories'].values)\n",
    "\n",
    "\n",
    "features_tfidf.extend(vectorizer_sub_cat.get_feature_names\n",
    "                      ())\n",
    "features_std.extend(vectorizer_sub_cat.get_feature_names())\n",
    "features.extend(vectorizer_sub_cat.get_feature_names())\n",
    "\n",
    "print(vectorizer_sub_cat.get_feature_names())\n",
    "print(\"Shape of matrix X_train_sub_categories_one_hot  after one hot encodig \",X_train_sub_categories_one_hot.shape)\n",
    "print(\"Shape of matrix X_test_sub_categories_one_hot after oneX_test_sub_categories_one_hot  hot encodig \",X_test_sub_categories_one_hot.shape)\n",
    "print(\"Shape of matrixX_cv_sub_categories_one_hot after one hot encodig \",X_cv_sub_categories_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can do the similar thing with state, teacher_prefix and project_grade_category also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Vectorizing Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
    "# make sure you have the glove_vectors file\n",
    "with open('C:\\\\VipinML\\\\InputData\\\\glove_vectors', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words =  set(model.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of preprocessed_essays for X_train,X_test, X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 897/897 [00:00<00:00, 2104.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_train_avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_train_preprocessed_essays): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    X_train_avg_w2v_vectors.append(vector)\n",
    "\n",
    "print(len(X_train_avg_w2v_vectors))\n",
    "print(len(X_train_avg_w2v_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:00<00:00, 2078.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_test_avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_test_preprocessed_essays): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    X_test_avg_w2v_vectors.append(vector)\n",
    "\n",
    "print(len(X_test_avg_w2v_vectors))\n",
    "print(len(X_test_avg_w2v_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 443/443 [00:00<00:00, 1575.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_cv_avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_cv_preprocessed_essays): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    X_cv_avg_w2v_vectors.append(vector)\n",
    "\n",
    "print(len(X_cv_avg_w2v_vectors))\n",
    "print(len(X_cv_avg_w2v_vectors[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of teacher_prefix  for X_train,X_test, X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['College_CareerPrep' 'Literacy SpecialNeeds'\n",
      " 'Literacy Literature_Writing' 'Literature_Writing Mathematics'\n",
      " 'Literacy Music' 'EnvironmentalScience Mathematics' 'Health_Wellness'\n",
      " 'Literacy Literature_Writing' 'Literacy SpecialNeeds'\n",
      " 'Literature_Writing' 'Literacy' 'AppliedSciences' 'Literacy Mathematics'\n",
      " 'SocialSciences' 'Mathematics' 'Literacy' 'ESL VisualArts'\n",
      " 'Literacy Literature_Writing' 'Literacy Literature_Writing'\n",
      " 'AppliedSciences VisualArts' 'CharacterEducation Other'\n",
      " 'Literacy SpecialNeeds' 'Health_Wellness' 'Mathematics' 'Gym_Fitness'\n",
      " 'Literacy' 'Literacy' 'Literacy Mathematics'\n",
      " 'EnvironmentalScience VisualArts' 'Literature_Writing Mathematics'\n",
      " 'Health_Wellness' 'Literacy Literature_Writing' 'Literacy Mathematics'\n",
      " 'History_Geography Literacy' 'Literacy' 'Literacy' 'Mathematics'\n",
      " 'VisualArts' 'Literature_Writing Mathematics'\n",
      " 'Literacy Literature_Writing' 'Gym_Fitness Health_Wellness'\n",
      " 'Literature_Writing SocialSciences' 'Gym_Fitness Health_Wellness'\n",
      " 'Literacy' 'College_CareerPrep Literature_Writing' 'Literacy'\n",
      " 'Literature_Writing Mathematics' 'EnvironmentalScience Literacy'\n",
      " 'AppliedSciences Literacy' 'AppliedSciences Civics_Government'\n",
      " 'FinancialLiteracy' 'VisualArts' 'ESL Literacy' 'Mathematics'\n",
      " 'Health_LifeScience Mathematics' 'Literacy' 'Other'\n",
      " 'EarlyDevelopment SpecialNeeds' 'Mathematics'\n",
      " 'AppliedSciences History_Geography' 'SpecialNeeds TeamSports'\n",
      " 'TeamSports' 'Extracurricular PerformingArts'\n",
      " 'Literature_Writing Mathematics' 'SpecialNeeds' 'Health_Wellness'\n",
      " 'Literature_Writing Mathematics' 'PerformingArts VisualArts'\n",
      " 'AppliedSciences Mathematics' 'EarlyDevelopment'\n",
      " 'Literacy Literature_Writing' 'Health_Wellness' 'Health_Wellness'\n",
      " 'Health_LifeScience' 'Literacy Mathematics' 'AppliedSciences Mathematics'\n",
      " 'Health_LifeScience Mathematics' 'Literacy' 'SpecialNeeds'\n",
      " 'EnvironmentalScience Mathematics'\n",
      " 'EnvironmentalScience Health_LifeScience' 'Literacy Mathematics'\n",
      " 'Literacy Mathematics' 'Health_Wellness NutritionEducation'\n",
      " 'Health_LifeScience' 'Health_Wellness'\n",
      " 'Health_Wellness Literature_Writing' 'AppliedSciences'\n",
      " 'Literacy Mathematics' 'Literature_Writing Mathematics'\n",
      " 'Warmth Care_Hunger' 'Health_LifeScience' 'Literacy Mathematics'\n",
      " 'Health_Wellness' 'Literacy Mathematics' 'AppliedSciences' 'VisualArts'\n",
      " 'Health_Wellness' 'Literacy Literature_Writing' 'Mathematics'\n",
      " 'Mathematics' 'Literacy' 'Music PerformingArts' 'Literacy'\n",
      " 'Literacy Mathematics' 'VisualArts' 'EarlyDevelopment'\n",
      " 'Health_Wellness SpecialNeeds' 'Mathematics' 'Literature_Writing'\n",
      " 'Literacy Literature_Writing' 'Literacy' 'Health_LifeScience'\n",
      " 'CommunityService' 'Literature_Writing Mathematics'\n",
      " 'Gym_Fitness Health_Wellness' 'Literacy' 'Literacy Mathematics'\n",
      " 'Literature_Writing' 'Literacy' 'Mathematics SpecialNeeds'\n",
      " 'Health_Wellness Literacy' 'Literacy Mathematics'\n",
      " 'Health_Wellness SpecialNeeds' 'Literature_Writing Mathematics'\n",
      " 'Health_Wellness' 'EarlyDevelopment EnvironmentalScience'\n",
      " 'Literacy SpecialNeeds' 'Health_Wellness' 'Health_Wellness SpecialNeeds'\n",
      " 'Mathematics' 'Civics_Government History_Geography'\n",
      " 'AppliedSciences Mathematics' 'AppliedSciences Mathematics'\n",
      " 'Literacy Mathematics' 'SpecialNeeds VisualArts' 'Music'\n",
      " 'EarlyDevelopment Other' 'Literature_Writing SpecialNeeds'\n",
      " 'AppliedSciences Mathematics' 'AppliedSciences Mathematics'\n",
      " 'Gym_Fitness Health_Wellness' 'Mathematics' 'Literacy Mathematics'\n",
      " 'ESL Literature_Writing' 'ESL Literature_Writing'\n",
      " 'AppliedSciences EarlyDevelopment' 'Literature_Writing VisualArts'\n",
      " 'Literacy Mathematics' 'Literature_Writing' 'Literacy Literature_Writing'\n",
      " 'Literature_Writing SpecialNeeds' 'Literacy' 'Health_Wellness' 'Literacy'\n",
      " 'AppliedSciences EarlyDevelopment' 'Mathematics' 'Health_Wellness'\n",
      " 'Mathematics' 'Literature_Writing Mathematics' 'Health_Wellness'\n",
      " 'Literacy' 'PerformingArts' 'History_Geography Literature_Writing'\n",
      " 'ESL Literature_Writing' 'Warmth Care_Hunger' 'Literature_Writing'\n",
      " 'Literacy Literature_Writing' 'Music PerformingArts'\n",
      " 'Gym_Fitness TeamSports' 'EnvironmentalScience Literacy'\n",
      " 'EarlyDevelopment Literacy' 'Literacy Mathematics' 'Literature_Writing'\n",
      " 'College_CareerPrep' 'College_CareerPrep Literacy' 'ESL Literacy'\n",
      " 'Health_Wellness SpecialNeeds' 'AppliedSciences Literature_Writing'\n",
      " 'Literacy' 'Literacy' 'History_Geography' 'SpecialNeeds'\n",
      " 'Economics FinancialLiteracy' 'Mathematics' 'Gym_Fitness Health_Wellness'\n",
      " 'Literature_Writing Mathematics' 'EnvironmentalScience VisualArts'\n",
      " 'Mathematics SpecialNeeds' 'College_CareerPrep Literature_Writing'\n",
      " 'Literacy Literature_Writing' 'Literature_Writing Mathematics'\n",
      " 'Other SpecialNeeds' 'Health_LifeScience VisualArts' 'Literacy'\n",
      " 'Gym_Fitness TeamSports' 'Literature_Writing' 'Music' 'Mathematics'\n",
      " 'Literature_Writing' 'ESL Mathematics'\n",
      " 'EarlyDevelopment Literature_Writing' 'SpecialNeeds'\n",
      " 'Literature_Writing PerformingArts' 'Gym_Fitness Health_Wellness'\n",
      " 'AppliedSciences' 'Health_Wellness' 'Warmth Care_Hunger'\n",
      " 'Literature_Writing SpecialNeeds'\n",
      " 'EnvironmentalScience Health_LifeScience' 'AppliedSciences' 'Mathematics'\n",
      " 'Literacy Mathematics' 'AppliedSciences EarlyDevelopment'\n",
      " 'Mathematics VisualArts' 'ESL Mathematics' 'History_Geography'\n",
      " 'Warmth Care_Hunger' 'AppliedSciences' 'Literacy SpecialNeeds'\n",
      " 'Health_LifeScience Mathematics' 'Literacy Mathematics' 'Health_Wellness'\n",
      " 'Health_Wellness' 'AppliedSciences Mathematics'\n",
      " 'Literature_Writing Mathematics' 'Literacy' 'Music' 'AppliedSciences'\n",
      " 'Literacy' 'SpecialNeeds' 'ESL SpecialNeeds' 'AppliedSciences'\n",
      " 'Literature_Writing Mathematics' 'AppliedSciences EnvironmentalScience'\n",
      " 'Literature_Writing' 'Literacy Mathematics' 'Literacy Literature_Writing'\n",
      " 'PerformingArts' 'SpecialNeeds' 'Literature_Writing SpecialNeeds'\n",
      " 'Health_Wellness NutritionEducation' 'Literacy Literature_Writing'\n",
      " 'Gym_Fitness SpecialNeeds' 'Literacy' 'SpecialNeeds' 'Literature_Writing'\n",
      " 'Other SpecialNeeds' 'Literature_Writing VisualArts'\n",
      " 'Literacy Mathematics' 'Literacy Literature_Writing'\n",
      " 'History_Geography PerformingArts' 'Literature_Writing Mathematics'\n",
      " 'ESL Health_LifeScience' 'CommunityService Gym_Fitness'\n",
      " 'AppliedSciences Mathematics' 'Literature_Writing'\n",
      " 'EnvironmentalScience Mathematics' 'Music' 'ESL Literacy'\n",
      " 'AppliedSciences Mathematics' 'Literacy SpecialNeeds'\n",
      " 'AppliedSciences Mathematics' 'Health_LifeScience SpecialNeeds'\n",
      " 'College_CareerPrep Literacy' 'FinancialLiteracy Mathematics'\n",
      " 'Literature_Writing SocialSciences' 'Literacy Mathematics'\n",
      " 'Literacy Mathematics' 'Health_LifeScience' 'History_Geography'\n",
      " 'Literacy' 'CharacterEducation Mathematics' 'Gym_Fitness SpecialNeeds'\n",
      " 'ESL Literature_Writing' 'AppliedSciences Mathematics'\n",
      " 'Literacy Literature_Writing' 'AppliedSciences Other' 'Literacy'\n",
      " 'AppliedSciences Mathematics' 'Literature_Writing VisualArts'\n",
      " 'Literacy Mathematics' 'Music PerformingArts' 'EnvironmentalScience'\n",
      " 'Literacy Mathematics' 'AppliedSciences' 'Literacy'\n",
      " 'EnvironmentalScience Health_LifeScience' 'VisualArts' 'Mathematics'\n",
      " 'Literacy Mathematics' 'Health_LifeScience History_Geography'\n",
      " 'Mathematics SpecialNeeds' 'EnvironmentalScience Health_LifeScience'\n",
      " 'Health_Wellness SpecialNeeds' 'Literacy Literature_Writing'\n",
      " 'Literature_Writing' 'Mathematics' 'AppliedSciences EnvironmentalScience'\n",
      " 'Literacy SpecialNeeds' 'Mathematics' 'Gym_Fitness Health_Wellness'\n",
      " 'Literacy' 'Literacy' 'Mathematics SocialSciences'\n",
      " 'Literacy Literature_Writing' 'Literature_Writing' 'Mathematics'\n",
      " 'Literature_Writing SpecialNeeds' 'EnvironmentalScience Mathematics'\n",
      " 'Literature_Writing' 'EnvironmentalScience Health_LifeScience'\n",
      " 'Music PerformingArts' 'Literacy Mathematics' 'ESL Literacy'\n",
      " 'Literature_Writing SpecialNeeds' 'Literacy SpecialNeeds'\n",
      " 'Literature_Writing Mathematics' 'Mathematics' 'Mathematics' 'Literacy'\n",
      " 'EnvironmentalScience Mathematics' 'Health_Wellness NutritionEducation'\n",
      " 'College_CareerPrep SpecialNeeds' 'Literacy Mathematics' 'TeamSports'\n",
      " 'Literacy' 'Literacy Literature_Writing' 'Literacy' 'SpecialNeeds'\n",
      " 'Health_LifeScience Mathematics' 'Literacy Mathematics'\n",
      " 'Literature_Writing Other' 'Literacy' 'Mathematics'\n",
      " 'Literacy Literature_Writing' 'College_CareerPrep Literature_Writing'\n",
      " 'College_CareerPrep EarlyDevelopment' 'Literacy' 'ESL'\n",
      " 'Literature_Writing Mathematics'\n",
      " 'EnvironmentalScience Health_LifeScience' 'Literacy SocialSciences'\n",
      " 'AppliedSciences Mathematics' 'Literacy Mathematics' 'Literacy'\n",
      " 'Literacy Mathematics' 'Literature_Writing'\n",
      " 'CommunityService Literature_Writing' 'ESL Literacy' 'Literacy'\n",
      " 'Mathematics Music' 'Literacy Mathematics'\n",
      " 'Health_LifeScience Mathematics' 'Other' 'Health_Wellness Other'\n",
      " 'Literature_Writing' 'SpecialNeeds'\n",
      " 'EnvironmentalScience Health_LifeScience' 'AppliedSciences'\n",
      " 'Music PerformingArts' 'Gym_Fitness Health_Wellness'\n",
      " 'Literacy SpecialNeeds' 'Mathematics' 'Health_LifeScience Literacy'\n",
      " 'Health_Wellness TeamSports' 'Health_LifeScience Literacy'\n",
      " 'AppliedSciences' 'Music PerformingArts' 'Gym_Fitness' 'Mathematics'\n",
      " 'Mathematics' 'AppliedSciences Mathematics'\n",
      " 'AppliedSciences CharacterEducation' 'Gym_Fitness Health_Wellness'\n",
      " 'EarlyDevelopment' 'Other' 'Literacy' 'SpecialNeeds VisualArts'\n",
      " 'ESL Mathematics' 'Mathematics' 'Health_Wellness'\n",
      " 'CharacterEducation College_CareerPrep'\n",
      " 'Health_Wellness NutritionEducation' 'Literacy' 'Gym_Fitness'\n",
      " 'Literacy SocialSciences' 'ESL Literacy' 'Literature_Writing Mathematics'\n",
      " 'Literature_Writing Mathematics' 'ESL Literature_Writing' 'Mathematics'\n",
      " 'Literacy Literature_Writing' 'Literacy' 'Literacy' 'Music'\n",
      " 'Literacy SpecialNeeds' 'Gym_Fitness Health_Wellness' 'Mathematics'\n",
      " 'Literature_Writing Mathematics' 'Literacy'\n",
      " 'Literature_Writing Mathematics' 'Literature_Writing SpecialNeeds'\n",
      " 'Literacy' 'EnvironmentalScience Literacy' 'Literacy' 'Health_Wellness'\n",
      " 'Mathematics' 'TeamSports' 'AppliedSciences College_CareerPrep' 'Other'\n",
      " 'Health_Wellness SpecialNeeds' 'ESL ForeignLanguages' 'History_Geography'\n",
      " 'Literature_Writing' 'Literacy' 'Mathematics SpecialNeeds'\n",
      " 'Literacy Literature_Writing' 'Civics_Government Literacy'\n",
      " 'Gym_Fitness PerformingArts' 'Literacy Mathematics'\n",
      " 'Gym_Fitness ParentInvolvement' 'Literacy Literature_Writing'\n",
      " 'Mathematics' 'ESL Literacy' 'Literacy Mathematics' 'Mathematics'\n",
      " 'Health_Wellness SpecialNeeds' 'Extracurricular Literacy'\n",
      " 'Literature_Writing' 'Literacy Literature_Writing' 'AppliedSciences'\n",
      " 'SpecialNeeds' 'Literature_Writing Mathematics'\n",
      " 'AppliedSciences Mathematics' 'Music' 'Gym_Fitness SpecialNeeds'\n",
      " 'Literature_Writing VisualArts' 'VisualArts' 'Mathematics'\n",
      " 'Health_Wellness TeamSports' 'Other' 'Literacy SpecialNeeds'\n",
      " 'Literature_Writing Mathematics' 'Health_Wellness' 'Mathematics'\n",
      " 'Mathematics' 'Health_Wellness' 'Warmth Care_Hunger'\n",
      " 'AppliedSciences EarlyDevelopment'\n",
      " 'EnvironmentalScience Health_LifeScience' 'EarlyDevelopment SpecialNeeds'\n",
      " 'History_Geography Literacy' 'Literacy' 'Literacy SocialSciences'\n",
      " 'Literature_Writing SpecialNeeds' 'SpecialNeeds' 'EarlyDevelopment'\n",
      " 'Literacy SpecialNeeds' 'AppliedSciences Mathematics'\n",
      " 'Literature_Writing' 'Health_LifeScience VisualArts' 'Literacy'\n",
      " 'Other SpecialNeeds' 'Literature_Writing' 'Music' 'Literacy'\n",
      " 'ForeignLanguages Literature_Writing' 'FinancialLiteracy'\n",
      " 'Literacy Mathematics' 'AppliedSciences Mathematics'\n",
      " 'Literacy SpecialNeeds' 'Literacy Mathematics' 'EnvironmentalScience'\n",
      " 'Health_LifeScience' 'Literacy' 'Literacy'\n",
      " 'EarlyDevelopment PerformingArts' 'Literacy' 'ESL Literature_Writing'\n",
      " 'Literature_Writing' 'Literacy' 'AppliedSciences EnvironmentalScience'\n",
      " 'Literacy Mathematics' 'Music' 'SpecialNeeds' 'TeamSports'\n",
      " 'Mathematics SpecialNeeds' 'ForeignLanguages' 'Health_Wellness' 'Other'\n",
      " 'ForeignLanguages Literacy' 'Mathematics' 'Gym_Fitness Health_Wellness'\n",
      " 'Literature_Writing Mathematics' 'Mathematics'\n",
      " 'Literacy Literature_Writing' 'EarlyDevelopment SpecialNeeds'\n",
      " 'AppliedSciences Mathematics' 'ForeignLanguages Health_LifeScience'\n",
      " 'ESL Literacy' 'Literacy SpecialNeeds' 'Literacy Mathematics'\n",
      " 'Mathematics' 'Literacy Literature_Writing' 'Extracurricular Other'\n",
      " 'AppliedSciences Mathematics' 'Literacy'\n",
      " 'EnvironmentalScience Health_Wellness' 'Literature_Writing Mathematics'\n",
      " 'AppliedSciences Mathematics' 'AppliedSciences Mathematics'\n",
      " 'Literacy Literature_Writing' 'Mathematics'\n",
      " 'AppliedSciences EnvironmentalScience' 'AppliedSciences Other'\n",
      " 'AppliedSciences VisualArts' 'AppliedSciences ParentInvolvement'\n",
      " 'Literacy Mathematics' 'ESL Literacy' 'Health_LifeScience Mathematics'\n",
      " 'Literacy Mathematics' 'VisualArts' 'Literature_Writing Mathematics'\n",
      " 'Literature_Writing SocialSciences' 'AppliedSciences'\n",
      " 'ESL EarlyDevelopment' 'ESL SpecialNeeds' 'Literacy Mathematics'\n",
      " 'Mathematics VisualArts' 'Literacy Mathematics' 'Health_LifeScience'\n",
      " 'ESL Literacy' 'Literacy Literature_Writing' 'VisualArts'\n",
      " 'Health_LifeScience Health_Wellness' 'Literature_Writing Mathematics'\n",
      " 'Gym_Fitness SpecialNeeds' 'Health_Wellness SpecialNeeds'\n",
      " 'Health_Wellness' 'AppliedSciences EnvironmentalScience'\n",
      " 'Health_Wellness' 'Gym_Fitness TeamSports' 'Literacy'\n",
      " 'Literature_Writing Mathematics' 'AppliedSciences EnvironmentalScience'\n",
      " 'Literacy' 'Literacy Mathematics' 'EnvironmentalScience'\n",
      " 'Literature_Writing SpecialNeeds' 'AppliedSciences EnvironmentalScience'\n",
      " 'Other' 'AppliedSciences Mathematics' 'Health_Wellness'\n",
      " 'Literacy Mathematics' 'Health_LifeScience Health_Wellness'\n",
      " 'Literacy SpecialNeeds' 'EnvironmentalScience Literature_Writing'\n",
      " 'ESL Literature_Writing' 'Literacy SpecialNeeds'\n",
      " 'Literature_Writing SpecialNeeds' 'Literacy Mathematics'\n",
      " 'Literacy SpecialNeeds' 'TeamSports' 'Literature_Writing Mathematics'\n",
      " 'Literacy Literature_Writing' 'Gym_Fitness Health_Wellness'\n",
      " 'History_Geography Mathematics' 'Literature_Writing'\n",
      " 'Health_LifeScience Health_Wellness' 'AppliedSciences Mathematics'\n",
      " 'Music PerformingArts' 'AppliedSciences Health_LifeScience'\n",
      " 'Literature_Writing' 'AppliedSciences' 'TeamSports'\n",
      " 'CharacterEducation SpecialNeeds' 'Health_LifeScience' 'Literacy'\n",
      " 'Literature_Writing Mathematics' 'AppliedSciences EnvironmentalScience'\n",
      " 'SpecialNeeds' 'EnvironmentalScience Literature_Writing' 'SpecialNeeds'\n",
      " 'Mathematics' 'History_Geography Literature_Writing' 'Literacy'\n",
      " 'CharacterEducation SpecialNeeds' 'Music SocialSciences' 'TeamSports'\n",
      " 'Health_Wellness' 'Music' 'ESL Literacy' 'Warmth Care_Hunger'\n",
      " 'Health_LifeScience SpecialNeeds' 'Literacy SpecialNeeds'\n",
      " 'Health_Wellness' 'ForeignLanguages' 'Literacy Mathematics' 'Literacy'\n",
      " 'Literature_Writing Mathematics' 'AppliedSciences Literature_Writing'\n",
      " 'AppliedSciences EnvironmentalScience' 'Mathematics'\n",
      " 'EnvironmentalScience Mathematics' 'Gym_Fitness Health_Wellness'\n",
      " 'VisualArts' 'Literacy' 'VisualArts' 'Literature_Writing Mathematics'\n",
      " 'ESL Literature_Writing' 'Literacy' 'ESL Literature_Writing' 'Literacy'\n",
      " 'Civics_Government SocialSciences' 'ESL Literacy' 'Literacy Mathematics'\n",
      " 'Health_Wellness NutritionEducation' 'Literature_Writing Mathematics'\n",
      " 'Literature_Writing Mathematics' 'Literature_Writing SocialSciences'\n",
      " 'Literacy Literature_Writing' 'Literacy SpecialNeeds' 'Literacy'\n",
      " 'Gym_Fitness Health_Wellness' 'AppliedSciences'\n",
      " 'EnvironmentalScience Mathematics' 'Literacy' 'Literacy' 'Mathematics'\n",
      " 'Health_Wellness SpecialNeeds' 'Literacy' 'AppliedSciences Mathematics'\n",
      " 'Literacy SpecialNeeds' 'Gym_Fitness Health_Wellness'\n",
      " 'Literature_Writing' 'Literature_Writing Mathematics'\n",
      " 'Literacy Mathematics' 'Gym_Fitness NutritionEducation'\n",
      " 'Literacy SpecialNeeds' 'AppliedSciences Mathematics' 'Mathematics'\n",
      " 'Gym_Fitness' 'Gym_Fitness Health_Wellness'\n",
      " 'Literature_Writing SpecialNeeds' 'EnvironmentalScience Literacy'\n",
      " 'Literacy SpecialNeeds' 'AppliedSciences' 'Literacy'\n",
      " 'Literacy SpecialNeeds' 'Literacy' 'Literature_Writing'\n",
      " 'EarlyDevelopment Literature_Writing' 'Literacy Mathematics'\n",
      " 'Literature_Writing Mathematics' 'Health_Wellness TeamSports'\n",
      " 'ESL VisualArts' 'Literature_Writing' 'Literacy Mathematics'\n",
      " 'History_Geography Mathematics' 'Literature_Writing Mathematics'\n",
      " 'Literacy Literature_Writing' 'AppliedSciences' 'SpecialNeeds' 'Literacy'\n",
      " 'Warmth Care_Hunger' 'Literature_Writing SpecialNeeds'\n",
      " 'EnvironmentalScience' 'ESL Literature_Writing'\n",
      " 'Literature_Writing Mathematics' 'AppliedSciences Extracurricular'\n",
      " 'SocialSciences' 'Literature_Writing' 'Music' 'Literacy' 'SocialSciences'\n",
      " 'EarlyDevelopment' 'Literacy Mathematics'\n",
      " 'Health_LifeScience NutritionEducation' 'Literacy Mathematics'\n",
      " 'CharacterEducation EarlyDevelopment' 'Literature_Writing Mathematics'\n",
      " 'EnvironmentalScience' 'Mathematics' 'Literacy Mathematics'\n",
      " 'ESL Literacy' 'College_CareerPrep Other' 'SpecialNeeds' 'Literacy'\n",
      " 'Literature_Writing Mathematics' 'Literacy Mathematics'\n",
      " 'Literacy Mathematics' 'Literature_Writing Mathematics'\n",
      " 'Literacy Literature_Writing' 'CharacterEducation Literacy'\n",
      " 'Literacy Other' 'EarlyDevelopment Other' 'Extracurricular VisualArts'\n",
      " 'College_CareerPrep' 'Gym_Fitness Health_Wellness'\n",
      " 'PerformingArts VisualArts' 'Literacy Mathematics'\n",
      " 'AppliedSciences Mathematics' 'TeamSports' 'Literacy'\n",
      " 'Literature_Writing' 'Mathematics' 'Literature_Writing Mathematics'\n",
      " 'Gym_Fitness TeamSports' 'EnvironmentalScience Mathematics'\n",
      " 'AppliedSciences' 'Health_Wellness NutritionEducation' 'Mathematics'\n",
      " 'Civics_Government Literature_Writing'\n",
      " 'EnvironmentalScience Literature_Writing' 'Literacy' 'Literature_Writing'\n",
      " 'Literacy' 'SocialSciences VisualArts' 'CommunityService Extracurricular'\n",
      " 'Literacy' 'Gym_Fitness Health_Wellness' 'Warmth Care_Hunger' 'Literacy'\n",
      " 'Warmth Care_Hunger' 'Gym_Fitness Health_Wellness' 'Mathematics'\n",
      " 'Literacy' 'Health_Wellness NutritionEducation' 'Literacy Mathematics'\n",
      " 'Mathematics' 'Literature_Writing Mathematics'\n",
      " 'Gym_Fitness Health_Wellness' 'CharacterEducation Health_Wellness'\n",
      " 'Literacy' 'AppliedSciences Mathematics' 'Literature_Writing Mathematics'\n",
      " 'CharacterEducation SpecialNeeds' 'AppliedSciences' 'College_CareerPrep'\n",
      " 'Literacy' 'Civics_Government FinancialLiteracy'\n",
      " 'AppliedSciences Mathematics' 'Music PerformingArts'\n",
      " 'Gym_Fitness Health_Wellness' 'Literature_Writing' 'Literature_Writing'\n",
      " 'ESL Literacy' 'VisualArts' 'AppliedSciences Health_LifeScience'\n",
      " 'VisualArts' 'Warmth Care_Hunger' 'Literature_Writing Mathematics'\n",
      " 'SpecialNeeds' 'AppliedSciences' 'Literature_Writing'\n",
      " 'Literature_Writing' 'PerformingArts' 'AppliedSciences' 'SpecialNeeds'\n",
      " 'VisualArts' 'Mathematics' 'Literature_Writing Mathematics'\n",
      " 'Health_Wellness' 'CharacterEducation Literacy' 'TeamSports' 'TeamSports'\n",
      " 'VisualArts' 'Health_Wellness SpecialNeeds' 'Gym_Fitness Health_Wellness'\n",
      " 'Literacy Literature_Writing' 'Literacy Mathematics'\n",
      " 'Civics_Government Literature_Writing' 'Literacy Mathematics'\n",
      " 'Literacy Mathematics' 'PerformingArts' 'ESL' 'Literacy' 'Mathematics'\n",
      " 'Music PerformingArts' 'Health_Wellness TeamSports'\n",
      " 'CharacterEducation VisualArts' 'Literacy Mathematics'\n",
      " 'Literature_Writing Mathematics' 'Literacy' 'Literacy Mathematics'\n",
      " 'Music' 'Gym_Fitness' 'Music' 'Literacy Literature_Writing'\n",
      " 'SpecialNeeds' 'AppliedSciences Mathematics' 'Literacy Mathematics'\n",
      " 'Literacy' 'Mathematics' 'History_Geography Literature_Writing'\n",
      " 'Literacy' 'Warmth Care_Hunger' 'EnvironmentalScience Health_LifeScience'\n",
      " 'Health_Wellness' 'Gym_Fitness NutritionEducation'\n",
      " 'Health_Wellness VisualArts' 'Health_Wellness SpecialNeeds'\n",
      " 'AppliedSciences Mathematics' 'AppliedSciences' 'AppliedSciences'\n",
      " 'Literacy VisualArts' 'Mathematics' 'EarlyDevelopment Literacy'\n",
      " 'Health_LifeScience Literature_Writing'\n",
      " 'AppliedSciences College_CareerPrep' 'EnvironmentalScience Literacy'\n",
      " 'AppliedSciences EnvironmentalScience' 'Literacy SpecialNeeds'\n",
      " 'AppliedSciences VisualArts' 'Literacy SpecialNeeds' 'Health_Wellness'\n",
      " 'Literacy' 'Mathematics' 'Literature_Writing Mathematics'\n",
      " 'AppliedSciences Literacy' 'Gym_Fitness Health_Wellness'\n",
      " 'Literacy SpecialNeeds' 'Civics_Government Literature_Writing'\n",
      " 'AppliedSciences EnvironmentalScience' 'Literature_Writing Mathematics'\n",
      " 'Gym_Fitness' 'Literacy SpecialNeeds' 'Literacy SpecialNeeds'\n",
      " 'Health_LifeScience Mathematics' 'Literature_Writing SpecialNeeds'\n",
      " 'Literacy' 'Literacy' 'Other VisualArts' 'Literature_Writing Mathematics'\n",
      " 'Literacy SpecialNeeds' 'Literacy Literature_Writing' 'ESL Literacy'\n",
      " 'Health_Wellness' 'Literature_Writing' 'Literacy Literature_Writing'\n",
      " 'Warmth Care_Hunger' 'Literacy Mathematics' 'Literacy'\n",
      " 'AppliedSciences Extracurricular' 'Literacy Literature_Writing'\n",
      " 'Literacy' 'AppliedSciences Mathematics'\n",
      " 'EnvironmentalScience Health_LifeScience' 'CharacterEducation Other'\n",
      " 'Gym_Fitness TeamSports' 'History_Geography Literacy'\n",
      " 'Literacy Mathematics' 'College_CareerPrep Literature_Writing'\n",
      " 'Literacy Other' 'Gym_Fitness TeamSports' 'Gym_Fitness Health_Wellness'\n",
      " 'Health_Wellness History_Geography' 'Literature_Writing'\n",
      " 'AppliedSciences Mathematics' 'Mathematics' 'AppliedSciences Mathematics'\n",
      " 'Literacy Literature_Writing' 'Literacy'\n",
      " 'AppliedSciences Health_LifeScience' 'Literature_Writing'\n",
      " 'Literature_Writing Mathematics' 'Literature_Writing Mathematics'\n",
      " 'TeamSports' 'Literacy Literature_Writing' 'Literacy' 'SpecialNeeds'\n",
      " 'College_CareerPrep Literature_Writing'\n",
      " 'College_CareerPrep PerformingArts' 'Health_Wellness NutritionEducation'\n",
      " 'Literacy Literature_Writing' 'SpecialNeeds'\n",
      " 'Literature_Writing Mathematics' 'Health_LifeScience' 'Literacy'\n",
      " 'ESL Literature_Writing' 'Other' 'SpecialNeeds' 'Health_Wellness'\n",
      " 'AppliedSciences EnvironmentalScience' 'Literacy Literature_Writing'\n",
      " 'Literacy' 'Literacy Literature_Writing' 'Health_LifeScience' 'Music'\n",
      " 'Health_Wellness Mathematics' 'Health_Wellness'\n",
      " 'Literacy Literature_Writing' 'VisualArts' 'Literacy Mathematics'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'Literacy' 'College_CareerPrep']\n",
      "Shape of matrix after one hot encodig  (897, 30)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one hot encoded features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_teacher_prefix = CountVectorizer(min_df=10,ngram_range=(1,4), vocabulary=list(sorted_teacher_dict.keys()),max_features=100, lowercase=False, binary=True)\n",
    "X_train_teacher_prefix_data = X_train['teacher_prefix']\n",
    "\n",
    "X_train_teacher_prefix_data.fillna(\"Mrs.\", inplace = True) \n",
    "\n",
    "teacher_prefix_notnull = X_train_teacher_prefix_data[pd.notnull(X_train_teacher_prefix_data)]\n",
    "\n",
    "vectorizer_teacher_prefix.fit(teacher_prefix_notnull.values)\n",
    "\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "print(teacher_prefix_notnull.values)\n",
    "\n",
    "X_train_teacher_prefix_one_hot = vectorizer_teacher_prefix.transform(teacher_prefix_notnull.values)\n",
    "print(\"Shape of matrix after one hot encodig \",X_train_teacher_prefix_one_hot.shape)\n",
    "\n",
    "features.extend(vectorizer_teacher_prefix.get_feature_names())\n",
    "features_tfidf.extend(vectorizer_teacher_prefix.get_feature_names())\n",
    "features_std.extend(vectorizer_teacher_prefix.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after one hot encodig  (660, 30)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one hot encoded features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_test_teacher_prefix_data = CountVectorizer(min_df=10,ngram_range=(1,4), max_features=100,vocabulary=list(sorted_teacher_dict.keys()), lowercase=False, binary=True)\n",
    "X_test_teacher_prefix_data = X_test['teacher_prefix']\n",
    "X_test_teacher_prefix_data.fillna(\"Mrs.\", inplace = True) \n",
    "teacher_prefix_notnull = X_test_teacher_prefix_data[pd.notnull(X_test_teacher_prefix_data)]\n",
    "vectorizer_test_teacher_prefix_data.fit(teacher_prefix_notnull.values)\n",
    "X_test_teacher_prefix_one_hot = vectorizer_test_teacher_prefix_data.transform(teacher_prefix_notnull.values)\n",
    "print(\"Shape of matrix after one hot encodig \",X_test_teacher_prefix_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Economics', 'ForeignLanguages', 'FinancialLiteracy', 'Extracurricular', 'CommunityService', 'ParentInvolvement', 'Civics_Government', 'Warmth', 'Care_Hunger', 'NutritionEducation', 'SocialSciences', 'CharacterEducation', 'History_Geography', 'PerformingArts', 'TeamSports', 'College_CareerPrep', 'Other', 'Music', 'ESL', 'EarlyDevelopment', 'Health_LifeScience', 'Gym_Fitness', 'VisualArts', 'EnvironmentalScience', 'AppliedSciences', 'Health_Wellness', 'SpecialNeeds', 'Literature_Writing', 'Mathematics', 'Literacy']\n",
      "Shape of matrix after one hot encodig  (443, 30)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one hot encoded features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_cv_teacher_prefix = CountVectorizer(min_df=10,ngram_range=(1,4), max_features=100,vocabulary=list(sorted_teacher_dict.keys()), lowercase=False, binary=True)\n",
    "X_cv_teacher_prefix_data = X_cv['teacher_prefix']\n",
    "X_cv_teacher_prefix_data.fillna(\"Mrs.\", inplace = True) \n",
    "teacher_prefix_notnull = X_cv_teacher_prefix_data[pd.notnull(X_cv_teacher_prefix_data)]\n",
    "vectorizer_cv_teacher_prefix.fit(teacher_prefix_notnull.values)\n",
    "print(vectorizer_cv_teacher_prefix.get_feature_names())\n",
    "X_cv_teacher_prefix_one_hot = vectorizer_cv_teacher_prefix.transform(teacher_prefix_notnull.values)\n",
    "print(\"Shape of matrix after one hot encodig \",X_cv_teacher_prefix_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of price for X_train,X_test, X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>school_state</th>\n",
       "      <th>Date</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>80893</td>\n",
       "      <td>p012896</td>\n",
       "      <td>d08b819b43647f8648a95ae4aa8ef68f</td>\n",
       "      <td>KY</td>\n",
       "      <td>2016-09-29 13:53:28</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>Back Jack Reading Chairs</td>\n",
       "      <td>My students come from a multitude of backgroun...</td>\n",
       "      <td>Having Back Jack reading chairs will help to c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need \\\"Back Jack\\\" chairs for read...</td>\n",
       "      <td>1</td>\n",
       "      <td>Literacy_Language</td>\n",
       "      <td>Literacy</td>\n",
       "      <td>Literacy</td>\n",
       "      <td>My students come from a multitude of backgroun...</td>\n",
       "      <td>103.01</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       id                        teacher_id school_state  \\\n",
       "998       80893  p012896  d08b819b43647f8648a95ae4aa8ef68f           KY   \n",
       "\n",
       "                   Date project_grade_category             project_title  \\\n",
       "998 2016-09-29 13:53:28             Grades 3-5  Back Jack Reading Chairs   \n",
       "\n",
       "                                       project_essay_1  \\\n",
       "998  My students come from a multitude of backgroun...   \n",
       "\n",
       "                                       project_essay_2 project_essay_3  \\\n",
       "998  Having Back Jack reading chairs will help to c...             NaN   \n",
       "\n",
       "    project_essay_4                           project_resource_summary  \\\n",
       "998             NaN  My students need \\\"Back Jack\\\" chairs for read...   \n",
       "\n",
       "     teacher_number_of_previously_posted_projects   clean_categories  \\\n",
       "998                                             1  Literacy_Language   \n",
       "\n",
       "    clean_subcategories teacher_prefix  \\\n",
       "998            Literacy       Literacy   \n",
       "\n",
       "                                                 essay   price  quantity  \n",
       "998  My students come from a multitude of backgroun...  103.01       4.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(1)\n",
    "X_test.head(1)\n",
    "X_cv.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(897, 1) (897,)\n",
      "(660, 1) (660,)\n",
      "(443, 1) (443,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "# normalizer.fit(X_train['price'].values)\n",
    "# this will rise an error Expected 2D array, got 1D array instead: \n",
    "# array=[105.22 215.96  96.01 ... 368.98  80.53 709.67].\n",
    "# Reshape your data either using \n",
    "# array.reshape(-1, 1) if your data has a single feature \n",
    "# array.reshape(1, -1)  if it contains a single sample.\n",
    "\n",
    "#normalizer.fit(X_train['price'].values.reshape(-1,1))\n",
    "\n",
    "X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(-1,1))\n",
    "X_test_price_norm = normalizer.transform(X_test['price'].values.reshape(-1,1))\n",
    "X_cv_price_norm = normalizer.transform(X_cv['price'].values.reshape(-1,1))\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_price_norm.shape, y_train.shape)\n",
    "print(X_test_price_norm.shape, y_test.shape)\n",
    "print(X_cv_price_norm.shape, y_cv.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words of project_title for X_train,X_test, X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix X_train_project_title_bow after one hot encodig  (897, 3525)\n",
      "Shape of matrix X_test_project_title_bow after one hot encodig  (660, 3525)\n",
      "Shape of matrix X_cv_project_title_bow after one hot encodig  (443, 3525)\n"
     ]
    }
   ],
   "source": [
    "# PROJECT_TITLE BOW\n",
    "# We are considering only the words which appeared in at least 10 documents(rows or projects). \n",
    "vectorizer_bow_project_title = CountVectorizer(min_df=10,ngram_range=(1,4),vocabulary=list(sorted_project_title_dict.keys()))\n",
    "X_train_project_title_bow = vectorizer_bow_project_title.fit_transform(X_train['project_title'])\n",
    "X_test_project_title_bow = vectorizer_bow_project_title.transform(X_test['project_title'])\n",
    "X_cv_project_title_bow = vectorizer_bow_project_title.transform(X_cv['project_title'])\n",
    "print(\"Shape of matrix X_train_project_title_bow after one hot encodig \",X_train_project_title_bow .shape)\n",
    "print(\"Shape of matrix X_test_project_title_bow after one hot encodig \",X_test_project_title_bow .shape)\n",
    "print(\"Shape of matrix X_cv_project_title_bow after one hot encodig \",X_cv_project_title_bow .shape)\n",
    "\n",
    "features.extend(vectorizer_bow_project_title.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words of preprocessed_essays for X_train,X_test, X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix X_train_text_bow after one hot encodig  (897, 18680)\n",
      "Shape of matrix X_test_text_bow after one hot encodig  (660, 18680)\n",
      "Shape of matrix X_cv_text_bow after one hot encodig  (443, 18680)\n"
     ]
    }
   ],
   "source": [
    "# We are considering only the words which appeared in at least 10 documents(rows or projects).\n",
    "vectorizer_bow_essays = CountVectorizer(min_df=10,ngram_range=(1,4), vocabulary=list(sorted_cat_essay_dict.keys()))\n",
    "X_train_text_bow = vectorizer_bow_essays.fit_transform(X_train_preprocessed_essays)\n",
    "X_test_text_bow = vectorizer_bow_essays.transform(X_test_preprocessed_essays)\n",
    "X_cv_text_bow = vectorizer_bow_essays.transform(X_cv_preprocessed_essays)\n",
    "\n",
    "features.extend(vectorizer_bow_essays.get_feature_names())\n",
    "\n",
    "print(\"Shape of matrix X_train_text_bow after one hot encodig \",X_train_text_bow.shape)\n",
    "print(\"Shape of matrix X_test_text_bow after one hot encodig \",X_test_text_bow.shape)\n",
    "print(\"Shape of matrix X_cv_text_bow after one hot encodig \",X_cv_text_bow.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF of preprocessed_essays for X_train,X_test, X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_model.fit(X_train_preprocessed_essays)\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n",
    "X_train_tfidf_words = set(tfidf_model.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF of preprocessed_essays for X_train,X_test, X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix X_train_text_tfidf after one hot encodig  (897, 8751)\n",
      "Shape of matrix X_test_text_tfidf after one hot encodig  (660, 8751)\n",
      "Shape of matrix X_cv_text_tfidf after one hot encodig  (443, 8751)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_tfidf_essays = TfidfVectorizer()\n",
    "X_train_text_tfidf = vectorizer_tfidf_essays.fit_transform(X_train_preprocessed_essays)\n",
    "X_test_text_tfidf = vectorizer_tfidf_essays.transform(X_test_preprocessed_essays)\n",
    "X_cv_text_tfidf = vectorizer_tfidf_essays.transform(X_cv_preprocessed_essays)\n",
    "print(\"Shape of matrix X_train_text_tfidf after one hot encodig \",X_train_text_tfidf.shape)\n",
    "print(\"Shape of matrix X_test_text_tfidf after one hot encodig \",X_test_text_tfidf.shape)\n",
    "print(\"Shape of matrix X_cv_text_tfidf after one hot encodig \",X_cv_text_tfidf.shape)\n",
    "\n",
    "features_tfidf.extend(vectorizer_tfidf_essays.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF of Project Title for X_train,X_test, X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix  X_train_project_title_tfidf after one hot encodig  (897, 82)\n",
      "Shape of matrix  X_test_project_title_tfidf after one hot encodig  (660, 82)\n",
      "Shape of matrix  X_cv_project_title_tfidf after one hot encodig  (443, 82)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_tfidf_project_title = TfidfVectorizer(min_df=10)\n",
    "X_train_project_title_tfidf = vectorizer_tfidf_project_title.fit_transform((X_train['project_title']))\n",
    "X_test_project_title_tfidf = vectorizer_tfidf_project_title.transform((X_test['project_title']))\n",
    "X_cv_project_title_tfidf = vectorizer_tfidf_project_title.transform((X_cv['project_title']))\n",
    "print(\"Shape of matrix  X_train_project_title_tfidf after one hot encodig \",X_train_project_title_tfidf.shape)\n",
    "print(\"Shape of matrix  X_test_project_title_tfidf after one hot encodig \",X_test_project_title_tfidf.shape)\n",
    "print(\"Shape of matrix  X_cv_project_title_tfidf after one hot encodig \",X_cv_project_title_tfidf.shape)\n",
    "\n",
    "features_tfidf.extend(vectorizer_tfidf_project_title.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF AVG W2V for Project Title for X_train,X_test, X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 897/897 [00:00<00:00, 61866.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_train_project_title_avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_train['project_title']): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    X_train_project_title_avg_w2v_vectors.append(vector)\n",
    "\n",
    "print(len(X_train_project_title_avg_w2v_vectors))\n",
    "print(len(X_train_project_title_avg_w2v_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:00<00:00, 67499.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_test_project_title_avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_test['project_title']): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    X_test_project_title_avg_w2v_vectors.append(vector)\n",
    "\n",
    "print(len(X_test_project_title_avg_w2v_vectors))\n",
    "print(len(X_test_project_title_avg_w2v_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 443/443 [00:00<00:00, 68973.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "X_cv_project_title_avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(X_cv['project_title']): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    X_cv_project_title_avg_w2v_vectors.append(vector)\n",
    "\n",
    "print(len(X_cv_project_title_avg_w2v_vectors))\n",
    "print(len(X_cv_project_title_avg_w2v_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(897, 61) (897,)\n",
      "(443, 61) (443,)\n",
      "(660, 61) (660,)\n",
      "====================================================================================================\n",
      "(897, 8894) (897,)\n",
      "(443, 8894) (443,)\n",
      "(660, 8894) (660,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# merge two sparse matrices: https://stackoverflow.com/a/19710648/4084039\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "X_tr = hstack((X_train_price_norm,X_train_sub_categories_one_hot,X_train_teacher_prefix_one_hot)).tocsr()\n",
    "X_cr = hstack((X_cv_price_norm,X_cv_sub_categories_one_hot,X_cv_teacher_prefix_one_hot)).tocsr()\n",
    "X_te = hstack((X_test_price_norm,X_test_sub_categories_one_hot,X_test_teacher_prefix_one_hot)).tocsr()\n",
    "\n",
    "#print (X_train_price_norm)\n",
    "X_tr_bow = hstack((X_train_price_norm,X_train_sub_categories_one_hot,X_train_teacher_prefix_one_hot,X_train_text_bow,X_train_project_title_bow)).tocsr()\n",
    "X_cr_bow = hstack((X_cv_price_norm,X_cv_sub_categories_one_hot,X_cv_teacher_prefix_one_hot,X_cv_text_bow,X_cv_project_title_bow)).tocsr()\n",
    "X_te_bow = hstack((X_test_price_norm,X_test_sub_categories_one_hot,X_test_teacher_prefix_one_hot,X_test_text_bow,X_test_project_title_bow)).tocsr()\n",
    "\n",
    "\n",
    "#print (X_train_price_norm)\n",
    "#X_tr_bow = hstack((X_train_price_norm,,X_train_teacher_prefix_one_hot,X_train_text_bow,X_train_project_title_bow)).tocsr()\n",
    "#X_cr_bow = hstack((X_cv_price_norm,,X_cv_teacher_prefix_one_hot,X_cv_text_bow,X_cv_project_title_bow)).tocsr()\n",
    "#X_te_bow = hstack((X_test_price_norm,,X_test_teacher_prefix_one_hot,X_test_text_bow,X_test_project_title_bow)).tocsr()\n",
    "\n",
    "\n",
    "X_tr_tfidf = hstack((X_train_sub_categories_one_hot,X_train_teacher_prefix_one_hot,X_train_price_norm,X_train_project_title_tfidf,X_train_text_tfidf)).tocsr()\n",
    "X_cr_tfidf = hstack((X_cv_sub_categories_one_hot,X_cv_teacher_prefix_one_hot,X_cv_price_norm,X_cv_project_title_tfidf,X_cv_text_tfidf)).tocsr()\n",
    "X_te_tfidf = hstack((X_test_sub_categories_one_hot,X_test_teacher_prefix_one_hot,X_test_price_norm,X_test_project_title_tfidf,X_test_text_tfidf)).tocsr()\n",
    "\n",
    "X_tr_tfidf_w2v = hstack((X_train_sub_categories_one_hot,X_train_teacher_prefix_one_hot,X_train_price_norm,X_train_project_title_avg_w2v_vectors)).tocsr()\n",
    "X_cr_tfidf_w2v = hstack((X_cv_sub_categories_one_hot,X_cv_teacher_prefix_one_hot,X_cv_price_norm,X_cv_project_title_avg_w2v_vectors)).tocsr()\n",
    "X_te_tfidf_w2v = hstack((X_test_sub_categories_one_hot,X_test_teacher_prefix_one_hot,X_test_price_norm,X_test_project_title_avg_w2v_vectors)).tocsr()\n",
    "\n",
    "\n",
    "X_tr_avg_w2v = hstack((X_train_sub_categories_one_hot,X_train_teacher_prefix_one_hot,X_train_price_norm,X_train_project_title_avg_w2v_vectors)).tocsr()\n",
    "X_cr_avg_w2v = hstack((X_cv_sub_categories_one_hot,X_cv_teacher_prefix_one_hot,X_cv_price_norm,X_cv_project_title_avg_w2v_vectors)).tocsr()\n",
    "X_te_avg_w2v = hstack((X_test_sub_categories_one_hot,X_test_teacher_prefix_one_hot,X_test_price_norm,X_test_project_title_avg_w2v_vectors)).tocsr()\n",
    "\n",
    "#print(\"Final Data matrix\")\n",
    "print(X_tr.shape, y_train.shape)\n",
    "print(X_cr.shape, y_cv.shape)\n",
    "print(X_te.shape, y_test.shape)\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(X_tr_tfidf.shape, y_train.shape)\n",
    "print(X_cr_tfidf.shape, y_cv.shape)\n",
    "print(X_te_tfidf.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li><strong>Apply Multinomial NaiveBayes on these feature sets</strong>\n",
    "        <ul>\n",
    "            <li><font color='red'>Set 1</font>: categorical, numerical features + project_title(BOW) + preprocessed_eassay (BOW)</li>\n",
    "            <li><font color='red'>Set 2</font>: categorical, numerical features + project_title(TFIDF)+  preprocessed_eassay (TFIDF)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li><strong>The hyper paramter tuning(find best Alpha)</strong>\n",
    "        <ul>\n",
    "    <li>Find the best hyper parameter which will give the maximum <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/receiver-operating-characteristic-curve-roc-curve-and-auc-1/'>AUC</a> value</li>\n",
    "    <li>Consider a wide range of alpha values for hyperparameter tuning, start as low as 0.00001</li>\n",
    "    <li>Find the best hyper paramter using k-fold cross validation or simple cross validation data</li>\n",
    "    <li>Use gridsearch cv or randomsearch cv or you can also write your own for loops to do this task of hyperparameter tuning</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li><strong>Feature importance</strong>\n",
    "        <ul>\n",
    "    <li>Find the top 10 features of positive class and top 10 features of negative class for both feature sets <font color='red'>Set 1</font> and <font color='red'>Set 2</font> using values of `feature_log_prob_` parameter of  <a href='https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html'>MultinomialNB</a> and print their corresponding feature names</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li><strong>Representation of results</strong>\n",
    "        <ul>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure. Here on X-axis you will have alpha values, since they have a wide range, just to represent those alpha values on the graph, apply log function on those alpha values.\n",
    "    <img src='train_cv_auc.JPG' width=300px></li>\n",
    "    <li>Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n",
    "    <img src='train_test_auc.JPG' width=300px></li>\n",
    "    <li>Along with plotting ROC curve, you need to print the <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/confusion-matrix-tpr-fpr-fnr-tnr-1/'>confusion matrix</a> with predicted and original labels of test data points. Please visualize your confusion matrices using <a href='https://seaborn.pydata.org/generated/seaborn.heatmap.html'>seaborn heatmaps.\n",
    "    <img src='confusion_matrix.png' width=300px></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li><strong>Conclusion</strong>\n",
    "        <ul>\n",
    "    <li>You need to summarize the results at the end of the notebook, summarize it in the table format. To print out a table please refer to this prettytable library<a href='http://zetcode.com/python/prettytable/'>  link</a> \n",
    "        <img src='summary.JPG' width=400px>\n",
    "    </li>\n",
    "        </ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>Note: Data Leakage</font></h4>\n",
    "\n",
    "1. There will be an issue of data-leakage if you vectorize the entire data and then split it into train/cv/test.\n",
    "2. To avoid the issue of data-leakag, make sure to split your data first and then vectorize it. \n",
    "3. While vectorizing your data, apply the method fit_transform() on you train data, and apply the method transform() on cv/test data.\n",
    "4. For more details please go through this <a href='https://soundcloud.com/applied-ai-course/leakage-bow-and-tfidf'>link.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Naive Bayes Validation </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.4 Appling NB() on different kind of featurization as mentioned in the instructions</h2>\n",
    "\n",
    "<br>Apply Naive Bayes on different kind of featurization as mentioned in the instructions\n",
    "<br> For Every model that you work on make sure you do the step 2 and step 3 of instrucations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(clf, data):\n",
    "    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "    # not the predicted outputs\n",
    "    y_data_pred = []\n",
    "    tr_loop = data.shape[0] - data.shape[0]%1000\n",
    "    # consider you X_tr shape is 49041, then your tr_loop will be 49041 - 49041%1000 = 49000\n",
    "    # in this for loop we will iterate unti the last 1000 multiplier\n",
    "    for i in range(0, tr_loop, 1000):\n",
    "        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])\n",
    "    # we will be predicting for the last data points\n",
    "    if data.shape[0]%1000 !=0:\n",
    "        y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n",
    "    \n",
    "    return y_data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are writing our own function for predict, with defined thresould\n",
    "# we will pick a threshold that will give the least fpr\n",
    "def find_best_threshold(threshould, fpr, tpr):\n",
    "    t = threshould[np.argmax(tpr*(1-fpr))]\n",
    "    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n",
    "    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n",
    "    return t\n",
    "\n",
    "def predict_with_best_t(proba, threshould):\n",
    "    predictions = []\n",
    "    for i in proba:\n",
    "        if i>=threshould:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomialNB_validation_alpha_analysis(X_tr,y_train,X_cr,y_cv):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \"\"\"\n",
    "    y_true : array, shape = [n_samples] or [n_samples, n_classes]\n",
    "    True binary labels or binary label indicators.\n",
    "\n",
    "    y_score : array, shape = [n_samples] or [n_samples, n_classes]\n",
    "    Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of\n",
    "    decisions (as returned by “decision_function” on some classifiers). \n",
    "    For binary y_true, y_score is supposed to be the score of the class with greater label.\n",
    "\n",
    "    \"\"\"\n",
    "    #alphas = np.logspace(-1, 100, num=10)\n",
    "    #alphas = [0.00001, 0.0006, 0.0009,0.0001, 0.0003, 0.0004, 0.0008, 0.001,0.01, 0.07,0.09, 0.1,0.2,0.3,0.4,0.5,0.9,1]\n",
    "    alphas =  [alpha for alpha in np.arange (0,1,0.01)]\n",
    "    train_auc = []\n",
    "    cv_auc = []\n",
    "    for i in tqdm(alphas):\n",
    "        nb = MultinomialNB(alpha=i,class_prior=[0.5,0.5])\n",
    "        nb.fit(X_tr, y_train)\n",
    "\n",
    "        y_train_pred = batch_predict(nb,X_tr)    \n",
    "        y_cv_pred = batch_predict(nb,X_cr)\n",
    "\n",
    "        # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "        # not the predicted outputs        \n",
    "        train_auc.append(roc_auc_score(y_train,y_train_pred))\n",
    "        cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\n",
    "\n",
    "    plt.plot(alphas, train_auc, label='Train AUC')\n",
    "    plt.plot(alphas, cv_auc, label='CV AUC')\n",
    "\n",
    "    plt.scatter(alphas, train_auc, label='Train AUC points')\n",
    "    plt.scatter(alphas, cv_auc, label='CV AUC points')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"alphas: hyperparameter\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.title(\"ERROR PLOTS\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-51-d72ac30fd875>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-51-d72ac30fd875>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    from sklearn.metrics import roc_auc_score()\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def multinomialNB_validation(X_tr,y_train,X_te,y_test):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.metrics import roc_auc_score()\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    \"\"\"\n",
    "    y_true : array, shape = [n_samples] or [n_samples, n_classes]\n",
    "    True binary labels or binary label indicators.\n",
    "\n",
    "    y_score : array, shape = [n_samples] or [n_samples, n_classes]\n",
    "    Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of\n",
    "    decisions (as returned by “decision_function” on some classifiers). \n",
    "    For binary y_true, y_score is supposed to be the score of the class with greater label.\n",
    "\n",
    "    \"\"\"\n",
    "    #alphas = np.logspace(-1, 100, num=10)\n",
    "    alphas = [0,0.00001, 0.0006, 0.0009,0.0001, 0.0003, 0.0004, 0.0008, 0.001,0.01, 0.07,0.09, 0.1,0.2,0.3,0.4,0.5,0.9,1]\n",
    "    nb = MultinomialNB()\n",
    "    train_auc = []\n",
    "    test_auc = []\n",
    "    grid = GridSearchCV(estimator=nb,param_grid=dict(alpha= alphas),return_train_score=True)\n",
    "\n",
    "    %time grid.fit(X_tr, y_train)\n",
    "    \n",
    "    print(\"Best grid score: %s\" % grid.best_score_)\n",
    "    print(\"Best grid estimator: %s\" % grid.best_estimator_.alpha)\n",
    "    \n",
    "    alpha =  grid.best_estimator_.alpha\n",
    "    \n",
    "    NB =   grid.best_estimator_\n",
    "    \n",
    "    print(\"Best alpha is after applying GridSearchCV: %s\" % alpha)\n",
    "\n",
    "    # 4. make class predictions for X_test_dtm\n",
    "    y_test_pred = grid.predict(X_te)    # 4. make class predictions for X_test_dtm\n",
    "    y_train_pred = grid.predict(X_tr)\n",
    "\n",
    "    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "    # not the predicted outputs        \n",
    "    train_auc.append(roc_auc_score(y_train,y_train_pred))\n",
    "    test_auc.append(roc_auc_score(y_test, y_test_pred))\n",
    "\n",
    "    # block starts\n",
    "    train_scores_mean = grid.cv_results_['mean_train_score']\n",
    "    train_scores_std =  grid.cv_results_['std_train_score']\n",
    "    test_scores_mean =  grid.cv_results_['mean_test_score']\n",
    "    test_scores_std =   grid.cv_results_['std_test_score']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title('Model')\n",
    "    plt.xlabel('$\\\\alpha$ (alpha)')\n",
    "    plt.ylabel('Score')\n",
    "    # plot train scores\n",
    "    plt.semilogx(alphas, train_scores_mean, label='Mean Train score',\n",
    "                 color='navy')\n",
    "    # create a shaded area between [mean - std, mean + std]\n",
    "    plt.gca().fill_between(alphas,\n",
    "                           train_scores_mean - train_scores_std,\n",
    "                           train_scores_mean + train_scores_std,\n",
    "                           alpha=0.2,\n",
    "                           color='navy')\n",
    "    plt.semilogx(alphas, test_scores_mean,\n",
    "                 label='Mean Test score', color='darkorange')\n",
    "\n",
    "    # create a shaded area between [mean - std, mean + std]\n",
    "    plt.gca().fill_between(alphas,\n",
    "                           test_scores_mean - test_scores_std,\n",
    "                           test_scores_mean + test_scores_std,\n",
    "                           alpha=0.2,\n",
    "                           color='darkorange')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    # block ends\n",
    "    \n",
    "    # calculate accuracy of class predictions\n",
    "    from sklearn import metrics\n",
    "    print (metrics.accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "    import seaborn as sns\n",
    "    conf_mat = confusion_matrix(y_test, y_test_pred)\n",
    "    conf_mat_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(conf_mat_normalized)\n",
    "    plt.ylabel('Test True label')\n",
    "    plt.xlabel('Test Predicted label')\n",
    "\n",
    "\n",
    "    #The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.\n",
    "    train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n",
    "    test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n",
    "\n",
    "    plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n",
    "    plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(\"AUC PLOT\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    import seaborn as sns\n",
    "    conf_mat = confusion_matrix(y_test, y_test_pred)\n",
    "    conf_mat_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(conf_mat_normalized)\n",
    "    plt.ylabel('Test True label')\n",
    "    plt.xlabel('Test Predicted label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to speculate the performance of the model using ROC Curve?\n",
    "#### An excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability. In fact it means it is reciprocating the result. It is predicting 0s as 1s and 1s as 0s. And when AUC is 0.5, it means model has no class separation capacity whatsoever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_best_validation(X_tr,y_tr,X_cv,y_cv,X_te, y_test,best_alpha,count_vect):\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "    # use model MultinomialNB, pass best aplha found already in earlier analysis.\n",
    "    nb = MultinomialNB(alpha=best_alpha,class_prior=[0.5,0.5])\n",
    "    nb.fit(X_tr, y_train)\n",
    "    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "    # not the predicted outputs\n",
    "\n",
    "    y_train_pred = batch_predict(nb, X_tr)    \n",
    "    y_test_pred = batch_predict(nb, X_te)\n",
    "\n",
    "    #The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.\n",
    "    train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n",
    "    test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n",
    "\n",
    "    # plot AUC curve. AUC curve should show best accuracy rate, since best aplha is used in the logic.\n",
    "    plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n",
    "    plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(\"AUC PLOT\")\n",
    "    plt.grid()\n",
    "    plt.show\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    import seaborn as sns\n",
    "    conf_mat = confusion_matrix(y_test, y_test_pred)\n",
    "    conf_mat_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(conf_mat_normalized)\n",
    "    plt.ylabel('Test True label')\n",
    "    plt.xlabel('Test Predicted label'\n",
    "    \n",
    "    neg_class_prob_sorted = nb.feature_log_prob_[0, :].argsort()\n",
    "    pos_class_prob_sorted = nb.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "   \n",
    "    print(\"Top 10 positive features %s\" % np.take(count_vect, pos_class_prob_sorted[:10]))\n",
    "    print(\"=\"*100)\n",
    "    print(\"Top 10 negative features %s\" % np.take(count_vect, neg_class_prob_sorted[:10]))\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NBAnalysis_cross_validation(X_tr,y_tr,X_cr,y_cv):\n",
    "    from sklearn import model_selection\n",
    "    from mlxtend.plotting import plot_decision_regions\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    # Import classification report and confusion matrix to evaluate predictions\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    # split the data set into train and test\n",
    "    # Break up the data as you see test data to 30%, train data is 70%. so we break data into two matrix. \n",
    "    # x1 and y1 would be 70% as train data and x-test and y_test will be test data as 30%.\n",
    " \n",
    "    error_rate = []\n",
    "    \n",
    "   # X_1, X_test, y_1, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    # split the train data set into cross validation train and cross validation test\n",
    "    #X_tr, X_cv, y_tr, y_cv = model_selection.train_test_split(X_1, y_1, test_size=0.3)\n",
    "\n",
    "    #Evaluate alternative K-values for better predictionsTo simplify the process of evaluating multiple cases of k-values, \n",
    "    #we create a function to derive the error using the average where our predictions were not equal to the test values.\n",
    "    for i in  [0.0000001,0.00001,0.0001, 0.001, 0.01, 0.06,0.07,0.1, 0.2,0.3,0.4,0.5,0.9,1,2]: \n",
    "        nb = MultinomialNB(alpha=i,class_prior=[0.5,0.5]) # Number of nearest neighbor is i.\n",
    "\n",
    "        # fitting the model on crossvalidation train\n",
    "        nb.fit(X_tr, y_tr)\n",
    "\n",
    "        # predict the response on the crossvalidation train\n",
    "        y_pred = nb.predict(X_cr)  # predicting the value using cross validation data. \n",
    "\n",
    "        # evaluate CV accuracy\n",
    "        acc = accuracy_score(y_cv, y_pred, normalize=True) * float(100)  # I get the accuracy score. \n",
    "        print('\\nCV accuracy for Alpha = %.6f is %d' % (i, acc))\n",
    "    \n",
    "        # evaluate CV accuracy\n",
    "        error_rate.append(np.mean(y_pred != y_cv))\n",
    "        #print (error_rate)\n",
    "   \n",
    "    # Configure and plot error rate over k values\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot([0.0000001,0.00001,0.0001, 0.001, 0.01, 0.06,0.07,0.1, 0.2,0.3,0.4,0.5,0.9,1,2], error_rate, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\n",
    "    plt.title('Error Rate vs. Alphas')\n",
    "    plt.xlabel('Alphas')\n",
    "    plt.ylabel('Error Rate')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.4 Appling MultiNomialNB on different kind of featurization as mentioned in the instructions</h2>\n",
    "\n",
    "<br>Apply KMultiNomialNBNN on different kind of featurization as mentioned in the instructions\n",
    "<br> For Every model that you work on make sure you do the step 2 and step 3 of instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.3 Make Data Model Ready: encoding eassay, and project_title</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.0  Alpha Validation  for MultinomialNB on   Set 1: categorical, numerical features + project_title(BOW) + preprocessed_essay (BOW),<font color='red'> SET 1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Alpha Validation  for MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB_validation_alpha_analysis(X_tr,y_train,X_te,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1  Alpha Validation  for MultinomialNB on  BOW - Set 1: categorical, numerical features + project_title(BOW) + preprocessed_essay (BOW),<font color='red'> SET 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write all the code with proper documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kbest for BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB_validation_alpha_analysis (X_tr_bow,y_train,X_cr_bow,y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multinomialNB_validation_alpha_analysis(X_tr_bow,y_train,X_cr_bow,y_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Alpha Validation  for MultinomialNB on TFIDF Set 2: categorical, numerical features + project_title(TFIDF)+ preprocessed_essay (TFIDF),<font color='red'> SET 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB_validation_alpha_analysis(X_tr_tfidf,y_train,X_cr_tfidf,y_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.1 <font color='red'> Alpha-Analysis  <font color='blue'> categorical, numerical features + project_title(BOW) + preprocessed_essay (BOW),<font color='red'> SET 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBAnalysis_cross_validation(X_tr_bow,y_train,X_cr_bow,y_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 <font color='red'> Alpha-Analysis   <font color='blue'>  on TFIDF Set 2: categorical, numerical features + project_title(TFIDF)+ preprocessed_essay (TFIDF),<font color='red'> SET 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBAnalysis_cross_validation(X_tr_tfidf,y_train,X_cr_tfidf,y_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 <font color='red'> ALPHA-BEST  <font color='blue'> categorical, numerical features + project_title(BOW) + preprocessed_essay (BOW),<font color='red'> SET 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best alpha is found from above analysis of BoW. using best Alpha, test accuracy is calcuated as around.\n",
    "Best_alpha=0.1\n",
    "NB_best_validation(X_tr_bow,y_train,X_cr_bow,y_cv,X_te_bow, y_test,Best_alpha,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 <font color='red'> Alpha-Best   <font color='blue'>  on TFIDF Set 2: categorical, numerical features + project_title(TFIDF)+ preprocessed_essay (TFIDF),<font color='red'> SET 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best alpha is found from above analysis of TfDIF. using best Alpha, test accuracy is calcuated as around.\n",
    "Best_alpha=0.7\n",
    "NB_best_validation(X_tr_tfidf,y_train,X_cr_tfidf,y_cv,X_te_tfidf, y_test,Best_alpha,features_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 <font color='red'> Alpha-Best   <font color='blue'>  on - categorical, numerical features + project_title + preprocessed_essay ,<font color='red'> SET 4</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best alpha is found from above analysis of Std. using best Alpha, test accuracy is calcuated as around.\n",
    "Best_alpha= 0.09\n",
    "NB_best_validation(X_tr,y_train,X_cr,y_cv,X_te, y_test,Best_alpha,features_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Conclusions</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of above program as below:\n",
    "### Step 1: Import the necessary Libraries\n",
    "we will need to import libraries that allow for data analysis and data visualization to get acclimated to the dataset. We will be using pandas, numpy, matplotlib and seaborn to conduct this. Data Exploration libraries\n",
    "\n",
    "### Step 2: Read in the dataset.\n",
    "We will use the pandas .read_csv() method to read in the dataset. Then we will use the. head() method to observe the first few rows of the data, to understand the information better. In our case, the feature(column) headers tell us pretty little. This is fine because we are merely trying to gain insight via classifying new data points by referencing it’s neighboring elements.\n",
    "\n",
    "### Step 3: Standardize (normalize) the data scale to prep for Multinomial NB algorithm.\n",
    "Because the distance between pairs of points plays a critical part on the classification, it is necessary to normalize the data This will generate an array of values. Again, KNN depends on the distance between each feature. Please see Section 1 for all normalization.\n",
    "\n",
    "### Step 4: Split the normalized data into training and test sets.\n",
    "This step is required to prepare us for the fitting (i.e. training) the model later. The “X” variable is a collection of all the features. The “y” variable is the target label which specifies the classification of 1 or 0 based. Our goal will be to identify which category the new data point should fall into.\n",
    "\n",
    "Please see functions as covered below, used in above program: def kanalysis_cross_validation(X,y): def knn_best_nfold_validation(X,y, k): def knn(X,y, k):\n",
    "\n",
    "### Step 5: Create and Train the Model.\n",
    "Here we create a KNN Object and use the .fit() method to train the model. Upon completion of the model we should receive confirmation that the training has been complete\n",
    "\n",
    "Please see functions as covered below, used in above program: def kanalysis_cross_validation(X,y): def knn_best_nfold_validation(X,y, k): def knn(X,y, k):\n",
    "\n",
    "### Step 6: Make Predictions.\n",
    "Here we review where our model was accurate and where it misclassified elements.\n",
    "\n",
    "Please see functions as covered below, used in above program: def kanalysis_cross_validation(X,y): def knn_best_nfold_validation(X,y, k):\n",
    "\n",
    "### Step 7: Evaluate the predictions.\n",
    "Evaluate the Model by reviewing the classification report or confusion matrix. By reviewing these tables, we are able to evaluate how accurate our model is with new values.\n",
    "\n",
    "def kanalysis_cross_validation(X,y): def knn_best_nfold_validation(X,y, k):\n",
    "\n",
    "### Setp 8:Classification Report :\n",
    "This tells us our model was around 84% accurate… Print out classification report and confusion matrix\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "I have covered various set to show confusion matrix.\n",
    "\n",
    "Please see section 2. covered various data sets and created confusion matrix.\n",
    "\n",
    "### Step 9: Evaluate alternative Alpha for better predictions.\n",
    "To simplify the process of evaluating multiple cases of Alpha values, we create a function to derive the error using the average where our predictions were not equal to the test values.\n",
    "\n",
    "Please see section 2. covered various data sets and created error accuracy reports.\n",
    "\n",
    "### Step 10: Plot Error Rate\n",
    "Here we see that the error rate continues to decrease as we increase the Alpha. A picture tells a thousand words. Or at least here, we are able to understand what value of Alpha leads to an optimal model. The Alpha of 1 or 0.9 seems to give a decent error rate without too much noise.\n",
    "\n",
    "### Step 11: Adjust Alpha value per error rate evaluations \n",
    "This is just fine tuning our model to increase accuracy. We will need to retrain our model with the new Alpha.\n",
    "Please see section 3 in above program. we have created confusion matrix for optimal Alpha value for various data sets. As we can see for optimal Alpha, Accuracy is much higher - so prediction is much better.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
